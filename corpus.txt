Use appropriate statistical techniques and predictive analytics on available data to deliver insights and discover new relations
Predictive analytics include statistical models and other empirical methods that are aimed at creating
empirical predictions, as well as methods for assessing the quality of those predictions in practice,
i.e., predictive power. Aside from their practical usefulness, predictive analytics play an important
role in theory building, theory testing, and relevance assessment. Hence, they are a necessary
component in scientific research (Kaplan 1964; Dubin 1969).
We show that despite prediction being a core scientific activity, empirical modeling in IS has been
dominated by causal-explanatory statistical modeling, where statistical inference is used to test causal
hypotheses and to evaluate the explanatory power of underlying causal models. Yet, contrary to
common belief, explanatory power does not imply predictive power (Dawes 1979; Forster and
Sober 1994). In addition, when statistical explanatory models are built for the purpose of testing

hypotheses rather than for generating accurate empirical predictions, they are less useful when the
main  is high predictive power.
The dominance of causal-explanatory statistical modeling and rarity of predictive analytics for theory
building and testing exists not only in IS but in the social sciences in general, as well as in other
disciplines such as economics and finance. In contrast, in fields such as computational linguistics and
bioinformatics, predictive analytics are commonly used and have lead to theoretical advances. In
computational linguistics: “the mathematical and computational work has given us deep insights into the working
of language... [and] will contribute to psycholinguistic research which studies the human processing of language”
(Joshi, 1991). In bioinformatics: “A predictive model represents the gold standard in understanding a biological
system and will permit us to investigate the underlying cause of diseases and help us to develop therapeutics” (Gifford,
2001).
We begin by defining the terms „explanatory statistical model‟ and „predictive analytics‟ and then
describe sources of differences between them. Next, the role of predictive analytics in scientific
research is discussed, followed by the results of an IS literature search indicating the rarity of
predictive analytics. The last part of the paper presents methods for assessing predictive power and
for building predictive models. The methods are illustrated by „converting‟ a well-known explanatory
study of TAM into a predictive context. We conclude with a discussion.
DEFINITIONS: “EXPLANATORY STATISTICAL MODELS” AND “PREDICTIVE
ANALYTICS”
We define „explanation‟ and „prediction‟ in the context of empirical modeling and the related terms
explanatory statistical model, explanatory power, predictive analytics, and predictive power.

Empirical Models for Explanation
In the context of empirical modeling, we use the term explanatory statistical model to describe a
statistical model that is built for the purpose of testing causal hypotheses that specify how and why
certain empirical phenomena occur (Gregor 2006). Causal theoretical models being at the core, a set
of causal hypotheses are then derived and tested using statistical models and statistical inference.
Explanatory statistical modeling includes two components:
(1) Explanatory statistical models, which include any type of statistical model used for testing causal
hypotheses. In IS, as in the social sciences in general, it is common to assume causality at the
theoretical level and then test causal hypotheses using association-type statistical models1 such
as regression models and structural equation models that rely on observational data.
(2) Methods for evaluating the explanatory power of a model (e.g., statistical tests or measures such as
R2), which indicates the strength of the relationship.
Examples of explanatory-oriented research in the IS literature, studied via explanatory statistical
modeling, include finding determinants of auction prices (Ariely and Simonson 2003); explaining the
diffusion and non-diffusion of e-commerce among SMEs (Grandon and Pearson 2004); explaining
attitudes towards  security and privacy (Malhotra et al. 2004); and understanding the
antecedents and consequences of  trust (Gefen et al. 2003).


Empirical Models for Prediction
Predictive analytics include two components:
(1) Empirical predictive models (statistical models and other methods such as data mining
algorithms) designed for predicting new/future observations or scenarios.
(2) Methods for evaluating the predictive power of a model.
Predictive power (or predictive accuracy2) refers to a model‟s ability to generate accurate predictions
of new observations, where „new‟ can be interpreted temporally, i.e., observations in a future time
period, or cross-sectionally, i.e., observations that were not included in the original sample used to
build the model. Examples of predictive-oriented research using predictive analytics in the context
of IS include predicting the price of ongoing eBay auctions (Wang et al. 2008a); predicting future
box-office sales based on  movie ratings (Dellarocas et al. 2006); and predicting repeat visits
and the likelihood of purchase of  customers (Padmanabhan et al. 2006).
Note that the above definition of prediction refers to empirical prediction rather than theoretical prediction,
where the latter describes an assertion that arises from a causal theory (e.g., “based on theory ABC,
we predict that X will be associated with Y” or “hypothesis H1 predicts that...”). In the remainder
of the paper, we use the terms “models”, “modeling”, and “prediction” in the sense of empirical
models, empirical modeling, and empirical prediction.
Empirical Models for Explanation and Prediction
Another theory type by Gregor (2006) is “Explanation and Prediction”. Both of these s are
desirable, and many empirical models indeed aim to achieve both. However, explanation and
prediction are perhaps best thought of as two separate modeling s not entirely mutually
exclusive, but with a tension between them. Since the best explanatory statistical model will almost


always greatly differ from the best predictive model (Forster and Sober 1994, Konishi and Kitagawa
2007, Shmueli 2010), any model that tries to achieve both s will have to somewhat compromise.
Such comprises are common and can take several forms. For instance, when the main purpose is
causal explanation but a certain level of predictive power is desired, one can build an explanatory
statistical model and then, in a second stage, assess its predictive power using predictive analytics,
perhaps modifying the model if it does not achieve the minimum desired level of predictive power.
Or, when the main purpose is prediction but a certain level of interpretability is required (e.g.,
because the logic underlying the model needs to be explained to stakeholders), then predictive
analytics can focus on predictors and methods that produce a relatively transparent model, while
perhaps sacrificing some predictive power. Hence, designing a model for both causal explanation
and empirical prediction requires understanding the tensions between the two s and the
difference between explanatory and predictive power.
In the remainder of the paper we focus on the distinction between explanatory statistical modeling
and predictive analytics. While we recognize the existence of modeling for a dual  as described
above, the exposition is eased if we present both types in their respective canonical forms to more
clearly dissipate the current ambiguity between them. This approach also helps highlight the roles
that predictive analytics play in scientific research, roles that differ yet complement those of
explanatory statistical modeling.
WHY EMPIRICAL EXPLANATION AND EMPIRICAL PREDICTION DIFFER
In the philosophy of science literature, there has been much debate over the difference between
“explaining” and “predicting” (e.g., Forster and Sober 1994; Forster 2002; Sober 2002; Hitchcock

and Sober 2004; Dowe et al. 2007). Dubin (1969) argued that predictive and explanatory s are
distinct, yet both are essential to scientific research:
Theories of social and human behavior address themselves to two distinct s of science: (1) prediction and
(2) understanding. It will be argued that these are separate s [...] I will not, however, conclude that they
are either inconsistent or incompatible. (Dubin, “Theory Building”, 1969, p.9)
In the context of IS research, Gregor (2006) proposed a taxonomy of five theory types, among them
“explanation”, “prediction”, and “explanation and prediction”.
We do not intend to contribute to the discussion at the philosophical level. Instead, we consider the
difference between explaining and predicting in the context of empirical modeling. Within this realm,
we emphasize two differences: (1) between explanatory and predictive modeling, and (2) between
explanatory power and predictive accuracy.
Statisticians recognize that statistical models aimed at explanation are different from those aimed at
prediction, and that explanatory power and predictive accuracy are two distinct dimensions of
empirical models. For example, Konishi and Kitagawa (2007, p.2) note,
There may be no significant difference between the point of view of inferring the true structure and that of
making a prediction if an infinitely large quantity of data is available [and] if the data are noiseless.
However, in modeling based on a finite quantity of real data, there is a significant gap between these two
points of view, because an optimal model for prediction purposes may be different from one obtained by
estimating the „true model‟.
In other words, the  of finding a predictively accurate model differs from the  of finding the
true model (see also Sober (2006, p.537)). Why does the  of analysis lead to such differences at
the empirical level? The reason is the different level on which the two types of empirical models
operate, as well as the notion of causality. Whereas explanatory statistical models are based on


underlying causal relationships between theoretical constructs, predictive models rely on associations between
measurable variables. The operationalization of theoretical models and constructs into empirical
models and measurable data creates a disparity between the ability to explain phenomena at the
conceptual level and to generate accurate predictions at the observed level.
A related fundamental difference between explanatory and predictive empirical modeling is the
metric optimized: whereas explanatory modeling seeks to minimize model bias (i.e., specification
error) to obtain the most accurate representation of the underlying theoretical model, predictive
modeling seeks to minimize the combination of model bias and sampling variance. However, there exists a
tradeoff between model bias and sampling variance (Geman et al. 1992; Friedman 1997), which
implies that improving predictive power sometimes requires sacrificing theoretical accuracy (higher
bias) for improved empirical precision (lower variance) (Hastie et al. 2008, p.57). Although a
properly specified explanatory statistical model will often exhibit some level of predictive power, the
large statistical literature on cross-validation, shrinkage, and over-fitting shows that the best-fitting
model for a single dataset is very likely to be a worse fit for future or other data (e.g., Stone 1974;
Copas 1983; Hastie et al. 2008). In other words, an explanatory model may have poor predictive
power, while a predictive model based on the same data may well possess high predictive power3.
Finally, the prospective nature of predictive modeling, where a model is built for predicting new
observations, is different from explanatory empirical modeling, where a model is built to
retrospectively test a set of existing hypotheses. One implication, for example, is that in a predictive
model all predictor variables must be available at the time of prediction, while in explanatory
modeling there is no such constraint. Consider the example of a linear regression model: although it


can be used for building an explanatory statistical model as well as a predictive model, the two
resulting models will differ in many ways. The differences are not only in the statistical criteria used
to assess the model, but are prevalent throughout the process of modeling: from the data used to
estimate the model (e.g., variables included and excluded, form of the variables, treatment of missing
data), to how performance is assessed (model validation and evaluation), and how results are used to
support research. We discuss and illustrate these and other issues in later sections.
Shmueli (2010) summarizes the aforementioned sources of differences between empirical
explanatory modeling and predictive analytics into four dimensions: causation-association, theory-
data, retrospective-prospective, and bias-variance. The theory-data dimension means that predictive
modeling relies more heavily on data whereas explanatory modeling relies more heavily on theory.
However, in the context of scientific research, the data-driven nature of predictive analytics is
integrated with theoretical knowledge throughout the entire model building and evaluation process,
albeit in a less formal way than in explanatory statistical modeling (see the Discussion for further
details and examples).
In summary, the different functions of empirical explanatory modeling and predictive analytics, and
the different contexts in which they are built and later operate (testing causal-theoretical hypotheses
versus generating data predictions), lead to many differences in the model building process, which
translate into different final models. The final models will differ in terms of explanatory power as
well as predictive power. Table 1 summarizes key differences that arise in explanatory and
predictive empirical modeling. A more detailed discussion of the differences that arise in the model
building process is presented in the section on Building Predictive Models.


Step
Analysis 
Variables of interest
Model building:
Optimized Function
Model building:
Constraints
Model evaluation
Description
Explanatory statistical models are used for testing causal hypotheses.
Predictive models are used for predicting new observations and assessing
predictability level.
Explanatory: operationalized variables are only used as instruments to
study the underlying conceptual constructs and the relation between them.
Predictive: The observed, measurable variables are the focus.
In explanatory modeling the focus is on minimizing model bias. Main risks
are type I and II errors. In predictive modeling the focus is on minimizing
the combined bias and variance. The main risk is over-fitting.
Explanatory: empirical model must be interpretable, must support
statistical testing of the hypotheses of interest, must adhere to theoretical
model (e.g., in terms of form, variables, specification).
Predictive: must use variables that are available at time of prediction.
Explanatory power is measured by strength-of-fit measures and tests (e.g.,
R2 and statistical significance of coefficients).
Predictive power is measured by accuracy of out-of-sample predictions.
THE ROLES OF PREDICTIVE ANALYTICS IN SCIENTIFIC RESEARCH
We now focus on the value of predictive analytics to theory building, theory testing, and relevance
assessment. We show that predictive analytics help develop and examine theoretical models through
a different lens than explanatory statistical models, and are therefore necessary in addition to
explanatory statistical models in scientific research. In particular, we describe six roles of predictive
analytics in research.
Role 1: Generating New Theory
The important role of predictive models in theory building is closely related to Glaser and Strauss‟s
argument, in the context of grounded theory, that both quantitative and qualitative data can be used
for theory building. The authors stress the importance of using quantitative data for generating new
theory: “quantitative data are often used not for rigorous demonstration of theory but as another
way to discover more theory” (Glaser and Strauss 1980, p. 235).



Predictive analytics are valuable for theory building especially in fast-changing environments, such as
the  environment, which poses many challenges for economic, psychological, and other
theoretical models traditionally employed in IS. An example is auctions, where classical auction
theory has only found limited applicability in the move from offline to  auctions, and where
empirical research of  auctions has raised new theoretical and practical questions that classical
auction theory does not address (Bajari and Hortacsu 2004; Bapna et al. 2008; Pinker et al. 2003).
The new types of datasets available today are rich in detail: they include and combine information of
multiple types (e.g., temporal, cross-sectional, geographical, and textual), on a large number of
observations, and with high level of granularity (e.g., clicks or bids at the seconds level). Such data
often contain complex relationships and patterns that are hard to hypothesize, especially given
theories that exclude many of newly-measurable concepts. Predictive analytics, which are designed
to operate in such environments, can detect new patterns and behaviors and help uncover potential
new causal mechanisms, in turn leading to the development of new theoretical models. One example
is the use of predictive analytics for forecasting prices of ongoing  auctions. The predictive
approach by Jank and Shmueli (2010, Chapter 4) relies on quantifying “price dynamics”, such as
price velocity and price acceleration patterns, from the auction start until the time of prediction, and
integrating these dynamics into a predictive model alongside other common predictors (e.g., item
characteristics and auction properties). While the concept of “price dynamics” is nonexistent in
classic auction theory, including such empirical measures in predictive models has been shown to
produce significantly more accurate price predictions across a range of items, auctions formats, and
marketplaces than models excluding such information. The predictive approach thus discovered the
new concept of “price dynamics” and its role in  auctions.
A second example is the study by Stern et al. (2004), in which predictive analytics were used to
detect factors affecting broadband adoption by Australian households, resulting in the discovery of a



new construct called “technophilia”. A third example is the work by Wang et al. (2008b), who
studied the relationship between how firms disclose security risk factors in a certain period and their
subsequent breach announcements. Using predictive analytics with textual data, the textual content
of security risk factors was found to be is a good predictor of future breaches, shedding light on a
relatively unexplored research area.
Role 2: Developing Measures
A second aspect of how predictive analytics support theory building is in terms of construct
operationalization. This aspect is a more specific instance of new theory generation, since the
development of new theory often goes hand in hand with the development of new measures (Van
Maanen et al. 2007; Compeau et al. 2007).
Predictive analytics can be used to compare different operationalizations of constructs, such as „user
competence‟ (e.g., Marcolin et al. 2000) or different measurement instruments. Szajna (1994) notes,
in the context of technology assessment instruments, that predictive validity provides a form of
construct validation. The study by Padmanabhan et al. (2006) used predictive analytics to show the
advantage of multi-source (user-centric) measures of user behavior over single-source (site-centric)
measures for capturing customer loyalty.
Role 3: Comparing Competing Theories
Given competing theoretical models, explanatory statistical models can be used as a means of
comparison. However, unless the theoretical models can be formulated in terms of nested statistical
models (i.e., one model contains another as a special case), it is difficult to compare them
statistically. Predictive analytics offer a straightforward way to compare models (whether explanatory
or predictive), by examining their predictive accuracy. The study on project escalation by Keil et al.
(2000) provides a good illustration of this aspect. They compared four explanatory (logistic



regression) models for testing the factors affecting project escalation, each model using constructs
from one of four theories (self-justification theory, prospect theory, agency theory, and approach
avoidance theory). All models exhibited similar explanatory power. The authors then proceeded to
test the predictive accuracy of the models using predictive analytics. They discovered that the
models based on approach avoidance and agency theories performed well in classifying both
escalated and non-escalated projects, while models based on self-justification and prospect theories
performed well only in classifying escalated projects, but did not perform well in their classification
of non-escalated projects. They further examined the different factors through the predictive lens
and discovered that the completion effect construct, derived from approach avoidance theory, had
high discriminatory power. Another example is the aforementioned study by Padmanabhan et al.
(2006), who used predictive analytics to identify factors impacting the gains from user-centric data.
A third example is the study by Collopy et al. (1994), who compared diffusion models with simpler
linear models for forecasting IS spending, and showed the higher predictive power of linear models.
Finally, Sethi and King (1999) used predictive analytics to compare linear and nar judgment
models for obtaining user information satisfaction (UIS) measures.
Role 4: Improving Existing Models
Predictive analytics can capture complex underlying patterns and relationships, and thereby improve
existing explanatory statistical models. One example is Ko and Osei-Bryson‟s study (2008)
examining the impact of investments in IT on hospital productivity. The authors chose predictive
analytics to resolve the mixed conclusions of previous explanatory models, and found that the
impact of IT investment was not uniform and that the rate of IT impact was contingent on the
amounts invested in the IT Stock, non-IT Labor, non-IT Capital, and possibly time.
 Their
predictive approach enabled them to capture the more complex non-linear nature of the
relationship, which in turn can be used to improve existing theoretical models (e.g., by including


moderated relationships). Another example is Keil et al.‟s (2000) study on determining the factors
that explain why some projects escalate and others do not. The authors, using predictive analytics to
test an explanatory model, discovered that using factors from self-justification and prospect theories
accurately predicted escalation, but poorly predicted non-escalation. Their finding indicates that
separate theoretical models are needed for escalation phenomena and non-escalation phenomena.
Such a theoretical nuance was not easily available from the explanatory metrics derived from the
explanatory statistical model, i.e. the statistical significance of the model and the coefficients for the
variables representing the different constructs.

Role 5: Assessing Relevance
Scientific development requires empirically rigorous and relevant research. In the words of Kaplan
(1964, p.350): “It remains true that if we can predict successfully on the basis of a certain
explanation, we have good reason and perhaps the best sort of reason, for accepting the
explanation”. Predictive analytics are useful tools for assessing the distance between theory and
practice. Although explanatory power measures can tell us about the strength of a relationship, they
do not quantify the empirical model‟s accuracy level in predicting new data. In contrast, assessing
predictive power can shed light on the actual performance of an empirical model.
The Keil et al. (2000) study described above also illustrates how predictive analytics can be used to
assess practical relevance. The authors found that the best model correctly classified 77% of the
escalated projects and 71% of the non-escalated projects. These values are practically meaningful, as
they give an idea of the impact of applying the theory in practice: how often will a project manager
be able to „see escalation coming‟ when using this model? When costs estimates of escalation and
non-escalation are available, practical relevance can be further quantified in monetary terms, which
could be used to determine the financial feasibility of preventive or corrective actions.


PREDICTIVE ANALYTICS IN THE INFORMATION SYSTEMS LITERATURE
To investigate the extent to which predictive analytics are integrated into mainstream empirical IS
research, a search of the literature was conducted. Using EBSCO‟s Business Source Premier, we
searched all fulltext articles in MIS Quarterly (MISQ) and Information Systems Research (ISR)
between 1990 - 20065 for one of the search terms “predictive OR predicting OR forecasting”. Initial
pre-testing of the search string revealed that although expanding the search to use additional terms
such as „predict‟, „prediction‟ or „predictor‟ yielded many more hits, none of the additional hits were
relevant for our purposes; All relevant items had already been captured by the more restrictive
search terms. The search returned a total of over 250 papers. Every article was then manually
examined for an explicit predictive , or for predictive claims made based on the empirical model.
We excluded articles that used predictive language in a generic sense (e.g., „based on theory ABC, we
predict that X will be associated with Y‟ or „hypothesis H1 predicts that...‟) as well as articles that
were qualitative or purely theoretical. We also excluded articles that, although explanatory in nature,
used the term “predictors” to denote covariates. These last comprised a majority of the papers
found. The total number of relevant predictive articles after the above filtering produced 52 articles
(18 in ISR and 34 in MISQ).
We subsequently investigated whether empirical papers with predictive claims evaluated predictive
power properly. The 52 articles were therefore checked for two distinguishing criteria of predictive
testing:
1.
 Was predictive accuracy based on out-of-sample assessment? (e.g., cross-validation or a
holdout sample). This criterion is well-established in predictive testing (see, e.g.,
Mosteller and Tukey 1977, and Collopy et al. 1994)
5
During this period there were a total of 692 articles published in MISQ and 380 in ISR.
17
2.
 Was predictive accuracy assessed with adequate predictive measures (e.g., RMSE,
MAPE, PRESS6, overall accuracy or other measures computed from a holdout set), or
was it incorrectly inferred from explanatory power measures? (e.g., p-values or R2)
It should be noted that both criteria are necessary for testing the predictive performance of any
empirical model, as they test predictive performance regardless of whether the  is explanatory
and/or predictive (see also next section on assessing predictive power).
Based on these criteria, each of the 52 articles was classified as one of four types (see Table 2):

 Predictive  - Adequate: predictive  stated; adequate predictive analytics used

 Predictive  - Inadequate : predictive  stated; inadequate predictive analytics used

 Predictive Assessment - Adequate : explanatory  stated; predictive power properly assessed

 Predictive Assessment - Inadequate : explanatory  stated; predictive power incorrectly inferred
from explanatory power
Table 2: Summary of Literature Search. Breakdown of “predictive” articles in ISR and
MISQ (1990-2006) according to predictive /claims and use of predictive analytics
ISR MISQ Total
Initial hits (predictive OR predicting OR forecasting, 1990-2006)
 95
 164
 259
Relevant papers (empirical with predictive  or claims of predictive
 18
 34
 52
power)
Predictive  - Adequate
 4
 1
 5
Predictive  - Inadequate
 8
 10
 18
Predictive Assessment – Adequate
 1
 1
 2
Predictive Assessment – Inadequate
 5
 22
 27
6
 RMSE = Root mean squared error. MAPE = Mean absolute percentage error. PRESS = predicted residual sum of
squares
18
Two major findings emerge from this literature study:
1.
 Empirical predictive s and claims are rare: From over 1,000 published articles, only 23 of
the empirical articles stated one or more s of analysis as predictive, and only 29 made
predictive claims regarding their explanatory model.
2.
 Predictive analytics are rare: Only 7 papers (out of the 52) employed predictive analytics in
one form or the other. The remaining 45 papers, although stating a predictive  or making
predictive claims, did not employ predictive analytics and instead inferred predictive power
from explanatory power. Table 4 in the appendix lists several illustrative quotes from articles
where measures of explanatory power are used for supporting predictive claims.
In summary, it can be seen from the literature search that predictive analytics are rare in mainstream
IS literature, and even when predictive s or statements about predictive power are made, they
incorrectly use explanatory models and metrics. This ambiguity between explanatory and predictive
empirical modeling and testing leads not only to ambiguity in matching methods to , but at
worst may result in incorrect conclusions for both theory and practice (e.g., Dawes 1979). Hence, we
next describe how predictive power should be evaluated and then describe the main steps and
considerations in building predictive models.
ASSESSING PREDICTIVE POWER (OF ANY EMPIRICAL MODEL)
Predictive power refers to an empirical model‟s ability to predict new observations accurately. In
contrast, explanatory power refers to the strength of association indicated by a statistical model. A
statistically-significant effect or relationship does not guarantee high predictive power, because the
precision or magnitude of the causal effect might not be sufficient for obtaining levels of predictive
accuracy that are practically meaningful. To illustrate a practical IS setting where this phenomenon
might occur, consider a TAM-based study on the acceptance of a radically new information system.
19
In such a setting, potential users have great uncertainty evaluating the usefulness of the system
(Hoeffler 2002), resulting in a much larger variance for the Perceived Usefulness (PU) construct.
While PU may still be statistically significant as in almost all TAM studies, its larger variance will
substantially reduce the gains in predictive accuracy from including it in the model, perhaps even to
the point of reducing predictive accuracy.
Most importantly, since the same data were used to fit the model and to estimate explanatory power,
performance on new data will almost certainly be weaker (Mosteller and Tukey 1977, p.37).
The first key difference between evaluating explanatory versus predictive power lies in the data used
for the assessment. While explanatory power is evaluated using in-sample strength-of-fit measures,
predictive power is evaluated using out-of-sample prediction accuracy measures. A popular method
to obtain out-of-sample data is to initially partition the data randomly, using one part (the „training
set‟) to fit the empirical model, and the other (the „holdout set‟) to assess the model‟s predictive
accuracy (Hastie et al. 2008, p.222; Berk 2008, p.31). In time series, the holdout set is chosen to be
the last periods of the series (see Collopy et al. 1994). With smaller datasets, where partitioning the
data can significantly deteriorate the fitted model (in terms of bias), methods such as cross-validation
are used. In cross-validation the model is fitted to the large majority of the data, and tested on a
small number of left-out observations. The procedure is then repeated multiple times, each time
leaving out a different set of observations, and finally the results from all repetitions are aggregated
to produce a measure of predictive accuracy (see Chapter 7.10 in Hastie et al. 2008 for further details
on cross-validation).
Low predictive power can result from “over-fitting”, where an empirical model fits the training data
so well that it underperforms in predicting new data (see Breiman 2001a, p.204). Hence, besides
avoiding fitting the training data too closely (Friedman, 2006), it is also important to compare the
20
model‟s performance on the training and holdout sets; a large discrepancy is indicative of over-
fitting, which will lead to low predictive accuracy on new data.
The second difference between explanatory and predictive power assessment is in the metrics used.
In contrast to explanatory power, statistical significance plays a minor role or no role at all in
assessing predictive performance. In fact, it is sometimes the case that removing predictors with
small coefficients, even if they are statistically significant (and theoretically justified), results in
improved prediction accuracy (see Wu et al. 2007; for a simple example see Appendix A in Shmueli
2010).
Similarly, R2 is an explanatory strength-of-fit measure, but does not indicate predictive accuracy (see,
e.g., Copas 1983, p.237; Berk 2008, p. 29). We especially note the widespread misconception of R2
as a predictive measure, as seen in our literature survey results (see Table 4) and even in textbooks
(e.g., Mendenhall and Sinich 1989, p.158). A model with a very high R2 indicates a strong
relationship within the data used to build that model, but the same model might have very low
predictive accuracy in practice (Barrett 1974).
In terms of predictive performance measures, popular metrics are out-of-sample error rate and
statistics such as PRESS, RMSE and MAPE or cross-validation summaries. A popular metric for
variable selection is the Akaike Information Criterion (AIC)7. Akaike derived the AIC from a
predictive viewpoint, where the model is not intended to accurately infer the “true distribution”, but
rather to predict future data as accurately as possible (see, e.g., Konishi and Kitagawa 2007 and Berk
2008). AIC is useful when maximum likelihood estimation is used, but otherwise too complicated to
compute.
7
 Although an in-sample metric, AIC is based on estimating the discrepancy between the in-sample and out-
of-sample error rate, and adding this discrepancy to the in-sample error (Hastie et al., 2001, p. 203).
21
Specialized predictive measures: When asymmetric costs are associated with prediction errors
(i.e., costs are heftier for some types of errors than for others), a popular measure is the “average
cost per predicted observation”. When the  is to accurately predict the “top tier” of a population
rather than the entire population (a  particularly common in marketing and personnel
psychology but often of interest in IS, e.g., predicting the most likely adopters of a new technology
or predicting the biggest barriers to successful IS implementation), then model building relies on all
observations, but predictive accuracy focuses on “top tier” observations, which will lead to a
different final model. Lift charts are commonly used in this context (see, e.g., Padmanabhan et al.
2006 and Shmueli et al. 2010). Note that due to its focus on a particular segment of the population,model with good lift need not necessarily exhibit a low overall error rate.
In short, since metrics for assessing predictive power are only based on the observed values and the
predicted values from the model, they can be evaluated for any empirical model that can generate
predictions. In contrast, since explanatory power assessment relies on statistical estimation and
statistical inference, assessing explanatory power is straightforward only with statistical models, but
not with every empirical model.
BUILDING PREDICTIVE MODELS
In this section we present a brief overview of steps and considerations in the process of building a
predictive model, which differ from explanatory statistical model building. We illustrate these in the
next section, by “converting” a known TAM explanatory study to a predictive context. For a
detailed exposition of the differences between predictive and explanatory model building from a
statistical methodological perspective see Shmueli (2010).
a
22
A schematic of the model building steps in explanatory and predictive modeling is shown in
Figure 1. Although the main steps are the same, within each step a predictive model dictates
different operations and criteria. The steps will now be described in more detail.
Figure 1: Schematic of the steps in building an empirical model (predictive or explanatory)
 Definition
One common  in predictive modeling is to accurately predict an outcome value for a new set of
observations. This  is known in predictive analytics as prediction (for a numerical outcome) or
classification (for a categorical outcome). A different , when the outcome is categorical (e.g.,
adopter/non-adopter), is to rank a new set of observations according to their probability of
belonging to a certain class, for the purpose of detecting the “top tier” (as mentioned earlier). This
 is known in predictive analytics as ranking.
Data Collection and Study Design
Experimental versus observational settings: Observational data can be preferable to “overly clean”
experimental data if they better represent the realistic context of prediction in terms of the
uncontrolled factors, the noise, the measured response and other factors. This is unlike explanatory
studies, where experiments are preferable for establishing causality (e.g., Rosenbaum 2002, p.11).
Data collection instrument: Focus is on measurement quality and relation to data at time of prediction.
In predictive analytics, closeness of the collected data (used for modeling) to the prediction context
is a main consideration. Ideally, the data used for modeling and for prediction consist of the same
variables and are drawn in a similar fashion from the same population. This consideration often
23
overrides explanatory considerations. For instance, whereas obtrusive collection methods are
disadvantageous in explanatory modeling due to the bias that they introduce, in predictive analytics
obtrusiveness is not necessarily problematic if the same instrument is employed at the time of
prediction. Similarly, secondary data (or even primary data) can be disadvantageous in predictive
analytics if they are too different from the measurements available at the time of prediction, even if
they represent the same underlying construct.
Sample size: In predictive analytics, required sample sizes are often larger than in explanatory
modeling for several reasons. First, predicting individual observations has higher uncertainty than
estimating population-level parameters (for instance, a confidence interval for the mean is narrower
than a prediction interval for a new observation). Second, the structure of the empirical model is
often learned directly from the data using data-driven algorithms rather than being constructed
directly by theory. Third, predictive analytics are often used to capture complex relationships.
Hence, increasing sample size can reduce both model bias and sampling variance. Finally, more data
are needed for creating holdout datasets to evaluate predictive power. Guidelines for minimum
sample size needed in predictive analytics are difficult to specify, as the required sample size depends
on the nature of the data, the properties of the final model, and the potential predictive power, all of
which are typically unknown at the start of the modeling process. Moreover, setting the sample size
apriori would limit the researcher‟s ability to use the wide range of available predictive tools or to
combine the results of multiple models, as is commonly done in predictive analytics.
Data dimension: The initial number of variables is usually large, in an effort to capture new sources of
information and new relationships. Justification for each variable is based on combining theory,
domain knowledge, and exploratory analysis. Large secondary datasets are often used in predictive
analytics due to their breadth.
24
Hierarchical designs: In hierarchical designs (e.g., a sample of students from multiple schools), sample
allocation for predictive purposes calls for increasing group size at the expense of the number of
groups (e.g., sample heavily in a small number of schools). This strategy is the opposite when the
 is explanatory (Afshartous and de Leeuw 2005).
Data Preparation
Missing values: Determining how to treat missing values depends on (1) whether the missingness is
informative of the response (Ding and Simonoff 2010) and (2) whether the missing values are in the
training set or in the to-be-predicted observations (Saar-Tsechansky and Provost 2007). Missingness
can be a blessing in a predictive context, if it is sufficiently informative of the response. For instance,
missing data for perceived usefulness in a TAM survey might be caused by a basic unfamiliarity with
the technology under investigation, which in turn increases the likelihood of non-adoption. Methods
for handling missing values include removing observations, removing variables, using proxy
variables, creating dummy variables that indicate missingness, and using algorithms such as
classification and regression trees for imputation. Note that this treatment of missing values in a
prediction context is different from that in the explanatory case, which is guided by other principles
(see Little and Rubin (2002)).
Data partitioning: The dataset is randomly partitioned into two parts. The „training set‟ is used to fit
models. A holdout set is used to evaluate predictive performance of the final chosen model. A third
dataset („validation set‟) might be used for model tuning and model selection (Hastie et al 2008, p.
222). If the dataset is too small for partitioning, cross-validation techniques can be used.
Exploratory Data Analysis (EDA)
EDA consists of summarizing data numerically and graphically, reducing their dimension, and
handling outliers.
25
Visualization: In predictive modeling, EDA is used in a free-form fashion to support capturing
relationships that are perhaps unknown or at least less formally formulated. This type of exploration
is called “exploratory visualization”, as opposed to the more restricted and theory-driven
“confirmatory visualization” (Fayyad et al. 2002). Interactive visualization supports exploration
across a wide and sometimes unknown terrain, and is therefore useful for learning about
measurement quality and associations that are at the core of predictive modeling.
Dimension reduction: Due to the often large number of predictors, reducing the dimension can help
reduce sampling variance (even at the cost of increasing bias), and in turn increase predictive
accuracy. Hence, methods such as principal components analysis (PCA) or other data compression
methods are often carried out initially. The compressed variables can then be used as predictors.
Choice of Variables
Predictive models are based on association rather than causation between the predictors and the
response. Hence variables (predictors and response) are chosen based on their observable qualities.
The response variable and its scale are chosen according to the predictive , data availability, and
measurement precision. Two constraints in choosing predictors are their availability at the time of
prediction (ex-ante availability8), and their measurement quality. The choice of potential predictors is
often wider than in an explanatory model, due to the objective of discovering new relationships.
Predictors are chosen based on a combination of theory, domain knowledge, and empirical evidence
of association with the response. Although in practical prediction the relation between the predictors
and underlying constructs is irrelevant, construct consideration can be relevant in some theoretical
development research (see Discussion section). Note that although improving construct validity
reduces model bias, it does not address measurement precision which affects sampling variance; and
8
 For instance, including the number of bidders in an  auction as a covariate is useful for explaining the final price,
but cannot be used for predicting the price of an ongoing auction (because it is unknown until the auction closes).
26
prediction accuracy is determined by both model bias and sampling variance. For this reason, when
proxy variables or even confounding variables can be measured more precisely and are more
strongly correlated with the measured output than “proper” causal variables, those can be better
choices for a predictive model than the “theoretically correct” predictors. For the same reason, in
predictive models there is typically no distinction between predictors in terms of their causal priority
as in mediation analysis, and considerations of endogeneity and model identifiability are irrelevant.
In fact, under-specified models can produce better predictions (Wu et al. 2007). For instance,
Montgomery et al. (2005) showed that it is often beneficial to exclude the main effects in a model
even if the interaction term between them is present.
Choice of Potential Methods
Data-driven algorithms: Predictive models often rely on non-parametric data mining algorithms (e.g.,
classification trees, neural networks and k-nearest-neighbors) and non-parametric smoothing
methods (e.g., moving average forecasters, wavelets). The flexibility of such methods enables them
to capture complex relationships in the data without making restricting statistical assumptions. The
price of this flexibility is lower transparency (“Unfortunately, in prediction, accuracy and simplicity
(interpretability) are in conflict.” Breiman 2001a, p.206). However, correct specification and model
transparency are of lesser importance in predictive analytics than in explanatory modeling.
Shrinkage methods: Methods such as ridge regression and principal components regression (Hastie et
al. 2008, Chapter 3) sacrifice bias for a reduction in sampling variance, resulting in improved
prediction accuracy (see, e.g., Friedman and Montgomery 1985). Such methods “shrink” predictor
coefficients or even set them to zero, thereby effectively removing the predictors altogether.
Ensembles: A popular method for improving prediction accuracy is using ensembles, i.e., averaging
across multiple models that rely on different data or reweighted data, and/or employ different
27
models or methods. Similar to financial asset portfolios („ensembles‟), where a reduction of portfolio
risk can be achieved through diversification, the underlying idea of ensembles is that combining
models reduces the sampling variance of the final model, which results in better predictions. Widely-
used ensemble methods include bagging (Breiman, 1996), random forests (Breiman, 2001b),
boosting (Shapire, 1999), and variations of these methods.
Evaluation, Validation and Model Selection
Model Evaluation: To evaluate the predictive performance of a model, predictive accuracy is measured
by applying the method to a holdout set and generating predictions.
Model Validation: Over-fitting is the major focus in predictive analytics (Stone 1974, Copas 1983,
Hastie et al. 2008). Assessing over-fitting is achieved by comparing the performance on the training
and holdout sets, as described earlier.
Model Selection: One way to reduce sampling variance is to reduce the data dimension (number of
predictors). Model selection is aimed at finding the right level of model complexity that balances bias
and variance, in order to achieve high predictive accuracy. This consideration is different from
explanatory considerations such as model specification. For instance, for purposes of prediction
“multicollinearity is not so damning” (Vaughan and Berry 2005, ). Variable selection and stepwise-
type algorithms are useful, as long as the selection criteria are based on predictive power (i.e., using
predictive metrics as described in the section on Assessing Predictive Power).
Model Use and Reporting
Studies that rely on predictive analytics focus on predictive accuracy and its meaning. Performance
measures (e.g., error rates and classification matrices) and plots (e.g., ROC curves and lift charts) are
geared towards conveying predictive accuracy and if applicable, related costs. Predictive power is
compared against naive and alternative predictive models (e.g., Armstrong 2001). In addition, the


treatment of over-fitting is often discussed. An example of a predictive study report in the IS
literature is Padmanabhan et al. (2006). Note the overall structure: the placement of the section on
„Rational behind Variable Construction‟ in the Appendix; the lack of causal statements or
hypotheses; the reported measures and plots; the emphasis on predictive assessment; reporting
model evaluation in practically relevant terms; and the translation of results into new knowledge.
EXAMPLE: PREDICTIVE MODEL FOR TAM
To illustrate how the considerations mentioned above affect the process of building a predictive
model, and to contrast that with the explanatory process, we will „convert‟ a known IS explanatory
study into a predictive one in the context of the TAM model (Davis 1989). In particular, we chose
the study on “Trust and TAM in  Shopping: An Integrated Model” by Gefen et al. (MISQ,
2003) – further denoted GKS. In brief, the study examines the role of trust and IT assessment
(perceived usefulness and ease of use) in  consumers‟ purchase intentions (denoted as
behavioral intention, or BI). The authors collected data via a questionnaire, filled by a sample of 400
students considered to be “experienced  shoppers”. Responders were asked about their last
 purchase of a CD or book. The final relevant dataset consisted of 213 observations and was
used to test a set of causal hypotheses regarding the effect of trust and IT assessment on purchase
intentions. The  of the GKS study was explanatory, and the statistical modeling was
correspondingly explanatory.
We now approach the same topic from a predictive perspective.
29
 Definition
Possible research s include benchmarking the predictive power of an existing explanatory TAM
model, evaluating the survey questions‟ ability to predict intention, revealing more complicated
relationships between the inputs and BI, and validating the predictive validity of constructs.
In terms of empirical , consider the  of predicting BI for shoppers that were not part of the
original sample. The original GKS data can be used as the training set to build (and evaluate) a
model that predicts BI. This model can then be deployed in a situation where a similar questionnaire
is administered to potential shoppers from the same population, but with the BI questions excluded
(whether to shorten questionnaire length, to avoid social desirability issues in answers, or for another
reason). According to his/her responses, the shopper‟s BI is predicted (and, for instance, an
immediate customization of the  store takes place).
The overall net benefit of the predictive model would be a function of the prediction accuracy and,
possibly, of costs associated with prediction error. For example, we may consider asymmetric costs,
such that erroneously predicting low BI (while in reality a customer has high BI) is more costly than
erroneously predicting high BI. The reason for such a cost structure could be the amount of effort
that an e-vendor invests in high-BI customers. Or, the opposite cost structure could be assumed, if
an e-vendor is focused on retention.
An alternative predictive  could be to rank a new set of customers from most likely to least likely
to express high BI, for the purpose of identifying, say, the top or bottom 10% of customers.
For the sake of simplicity, we continue with the first  described above, without considering
costs. Table 3 summarizes the main points and compares the explanatory and predictive modeling
processes that are described next.
30
Data Collection and Study Design
Experimental versus observational settings: Due to the predictive context, the GKS observational survey is
likely preferable to an experiment, because the “dirtier” observational context is similar to the
predictive context in the field than a “clean” lab setting would be.
Instrument: In choosing a data collection instrument, attention is first given to its relation to the
prediction context. For instance, using a survey to build and evaluate the model is most appropriate
if a survey will be used at the time of prediction. The questions and measurement scales should be
sufficiently similar to those used at the time of prediction. Moreover, the data to be predicted should
be from the same population as the training and evaluation data and should have similar sample
properties9, so that the training, evaluation and prediction contexts are as similar as possible. Note
that bias created by the obtrusive nature of the survey or by self-selection is irrelevant, because the
same mechanism would be used at the time of prediction. The suitability of a retrospective
questionnaire would also be evaluated in the prediction context, e.g., whether a retrospective recount
of a purchase experience is predictive of future BI. In designing the instrument, the correlation with
BI would also be taken into account (ideally through the use of pre-testing). For instance, the seven-
point Likert scale might be replaced by a different scale (finer or coarser) according to the required
level of prediction accuracy.
Sample size: The final usable sample of 213 observations is considered small in predictive analytics,
requiring the use of cross-validation in place of a holdout set and being limited to model-based
9
 The definition of „same population‟ is to some extent at the researcher‟s discretion, e.g., is the population
here „US college students‟, „college students‟, „experienced  shoppers‟, „ shoppers‟ and so on. The
population to which the predictive model is deployed should be similar to the one used for building and
evaluating the predictive model, otherwise predictive power is not guaranteed. In terms of sampling, if the
same biases (e.g., self-selection) are expected in the first and second datasets, then the predictive model can
be expected to perform properly. Finally, predictive assessment can help test the generalizability of the model
to other populations by evaluating predictive power on samples from such populations where the BI
questions are included, thereby serving as holdout samples.
31
methods. Depending on the signal strength and the data properties, a larger sample that would allow
for use of data-driven algorithms might improve predictive power.
Data dimension: Using domain knowledge and examining correlations, any additional information
beyond the survey answers that might be associated with BI would be considered, even if not
dictated by TAM theory (e.g., the website of most recent purchase, or # of previous purchases).
Data Preparation
Missing data: GKS report that the final dataset contained missing values. For prediction, one would
check whether the missingness is informative of BI, e.g., if it reflects less trusting behavior. If so,
including dummy variables that indicate the missingness might improve prediction accuracy.
Data partitioning: Due to the small dataset, the data would not be partitioned. Instead, cross-validation
methods would be used. When and if another sample is obtained (perhaps as more data are gathered
at the time of prediction), then the model could be applied to the new sample, which would be
considered a holdout set.
Exploratory Data Analysis (EDA)
Data visualization and summaries: Each question, rather than each construct, would be treated as an
individual predictor. In addition to exploring each variable, examining the correlation table between
BI and all the predictors would help identify strong predictor candidates and information overlap
between predictors (candidates for dimension reduction).
Dimension reduction: PCA or a different compression method would be applied to the predictors in
the complete training set, with predictors including individual questions and any other measured
variables such as demographics (this procedure differs from the explanatory procedure, where PCAs
were run separately for each construct.) The resulting compressed predictors would then be used in
the predictive model, with less or no emphasis on their interpretability or relation to constructs.


Choice of Variables
Ex-ante availability: To predict BI, predictors must be available at the time of prediction. The survey
asks respondents retrospectively about their perceived usefulness and ease of use as well as BI.
Given the predictive scenario, the model can be used for assessing the predictability of BI using
retrospective information, for comparing theories, or even for practical use. In either case, the BI
question(s) in the original study should be placed last in the questionnaire, to avoid affecting earlier
answers (a clickstream-based measure of BI, e.g. Hauser et al. (2009), would be another way of
dealing with this issue), and to obtain results that are similar to the prediction context. In addition,
each of the other collected variables should be assessed as to its availability at the time of prediction.
Measurement quality: The quality and precision of predictor measurements are of key importance in a
predictive version of GKS, but with a slight nuance: while a unidimensional operationalization of
constructs such as trust, PU and PEOU is desirable, it should not come at the expense of
measurement precision and hence increased variance. Unobtrusive measures such as clickstream
data or purchase history (if available) would be particularly valued here. Even though they might be
conceptually more difficult to interpret in terms of the underlying explanation, their measurement
precision can boost predictive accuracy.
Choice of Potential Methods
Data-driven algorithms would be evaluated (although the small dataset would limit the choices).
Shrinkage methods could be applied to the raw question-level data, before data reduction. If we are
predicting BI for people who have survey-answering profiles that are different from those in the
training data (i.e., extrapolation), then shrinkage methods should be used. The issue of extrapolation
is also relevant to the issue in GKS of generalizing their theory to other types of users.
33
Ensembles would be considered. In particular, the authors mention the two competing models of
TAM and TBP which can be averaged to produce an improved predictive model. Similarly, if
clickstream data were available, one could average the results from a survey-based BI-model and a
clickstream-based BI-model to produce improved predictions. If real-time prediction is expected
then computational considerations will affect the choice of methods.
Evaluation, Validation and Model Selection
Predictive variable selection algorithms (e.g., stepwise-type algorithms) could be used to reduce the
number of survey questions, using criteria such as AIC or out-of-sample predictive accuracy.
Predictive accuracy would be evaluated using cross-validation (due to small sample size), and
compared to competing models and the naïve prediction “predict each BI by the overall average BI”.
Model Use and Reporting
The results of the predictive analytics can be used here for one, or more of several research s:
(1) Benchmarking the predictive power of existing explanatory TAM models: The paper would
present the predictive accuracy of different TAM models and discuss practical differences. Also,
an indication of overall predictability could be obtained.
(2) Evaluating the actual precision of the survey questions with respect to predicting BI: A
comparison of the predictive accuracy of different models which rely on different questions.
(3) Revealing more complicated relationships between the inputs and BI, such as moderating
effects: Comparing the predictive power of the original and more complex model and showing
how the added complexity provides a useful contribution.
(4) Validating assertions about the predictive validity of concepts: GKS (p.73) remark: “the TAM
construct PU remains an important predictor of intended use, as in many past studies”. Such an
34
assertion in terms of actual prediction would be based on the predictive accuracy associated with
PU (e.g., by comparing the best model that excludes PU to the model with PU).
These are a few examples of how the predictive analytics complement explanatory TAM research.
Table 3: Buildingexample
Modeling
Step

Definition
Study
Design
and Data
Collection
Data
Preparation
explanatory versus predictive models: Summary of the Gefen et al. (2003)
Explanatory Task
 Predictive Task
Understand the role of trust and
 Predict the intention of use (BI) of new B2C
IT assessment (perceived
 website customers, or, predict 10% of those
usefulness and ease of use) in
 most likely to express high BI. (Might include
 consumers‟ purchase
 asymmetric costs).
intensions
Observational data
 Observational data – similar to prediction
context; Variables must be available at
prediction time
Survey (obtrusive)
Survey (obtrusive) – with identical questions and
Sample
 scales as at prediction time
size: 400 students (213 usable
Sample Size: Larger sample preferable
observations)
Variables: Predictors that strongly correlate with
Variables: operationalization of
BI (questions, demographics, other information)
PU and PEOU, demographics
Instrument: Questionnaire; BI questions last;
Instrument: Questionnaire; seven-non-retrospective would be better; scale for
point Likert scale
questions - according to required prediction
Pre-
 scale, and correlations with BI
testing:
 for
 validating
Pre-testing: for trouble-shooting questionnaire
questionnaire
Missing Values: some missing Missing Values: Is missingness informative of
values reported, action not BI? If so, add relevant dummy variables; is
reported
 missingness in training data or to-be-predicted
data?
Data
Partitioning: none
 Data Partitioning: sample size too small (213);
cross-validation used
Continued on Next page
35
Modeling
Step
Exploratory
Data
Analysis
Choice of
Variables
Choice of
Methods
Model
Evaluation,
Validation
and
Selection
Model Use
and
Reporting
Explanatory Task
 Predictive Task
Summaries: Numerical summaries Summaries: Examine numerical summaries of
for constructs; Pairwise
 all questions and additional collected variables
correlations between questions;
 (such as gender, age), correlation table with BI.
Univariate summaries by gender,
age and other variables.
Plots: Interactive visualization
Plots: None
Data Reduction: PCA or other data reduction
Data Reduction: PCA appliedmethod applied to complete set of questions
separately to each construct forand other variables; applied to entire data (not
purpose of construct validationjust pre-test)
(during pre-testing)
Guided by theoretical
 Predictors chosen based on their association
considerations
 with BI; BI chosen according to prediction 
Structural equations model (after Try an array of methods:
applying
 confirmatory
 factorModel-driven and data-driven methods,
analysis to validate the constructs)ideally on a larger collected sample: machine-
learning algorithms, parametric and non-
parametric statistical models
Shrinkage methods - for reducing dimension
(instead of PCA); for robust extrapolation (if
deemed necessary); for variable selection
Ensemble methods – combine several models
to improve accuracy, (e.g., TAM and TBP)
Questions
 removed
 from Variable selection algorithms applied to original
constructs based on residual questions
variance backed by theoretical
considerations; constructs included
based on theoretical considerations
Explanatory power based on
theoretical coherence, strength-of-
 Predictive power Predictive accuracy assessed
fit statistics, residual analysis,
 on holdout set (use cross-validation if small
estimated coefficients, statistical
 sample);
 evaluate
 over-fitting
 (compare
significance
 performance on training and holdout data)
Use: Test causal hypotheses about Use: Discover new relationships (e.g.,
how trust and TAM affect BI
 moderating effect; unexpected questions or
features that predict BI), evaluate magnitude of
Statistical Reporting:
trust and TAM effects in practice, assess
explanatory power metrics (e.g., predictability of BI
path
 coefficients),
 plot
 of
Statistical Reporting: predictive accuracy, final
estimated path model
predictors, method used, over-fitting analysis
36
DISCUSSION
In this essay we discussed the role of predictive analytics in scientific research; how they differ from
explanatory statistical modeling; and their current under-representation in mainstream IS literature.
We also described how to assess the predictive power of any empirical model and how to build a
predictive model. Predictive models can lead to the discovery of new constructs, new relationships,
nuances to existing models, and unknown patterns. Predictive assessment provides a straightforward
way to assess the practical relevance of theories, to compare competing theories, to compare
different construct operationalizations, and to assess the predictability of measurable phenomena.
Predictive analytics support the extraction of information from large datasets and from a variety of
data structures. Although they are more data-driven than explanatory statistical models, in the sense
that predictive models integrate knowledge from existing theoretical models in a less formal way
than explanatory statistical models, they can be useful for theory development provided that a
careful linkage to theory guides both variable and model selection. It is the responsibility of the
researcher to carefully ground the analytics in existing theory. The few IS papers that use predictive
analytics demonstrate the various aspects of linking and integrating the predictive analytics into
theory. One such link is in the literature review step, discussing existing theories and models and
how the predictive study fits in. Examples are the study by Ko and Osei-Bryson (2008) that relies on
production theory and considers existing IT productivity studies and models; The predictive work
by Wang et al. (2008b), which was linked to the body of literature in the two areas of management
and economics of information security and disclosures in accounting. And finally, the study by Stern
et al. (2004), which examined existing theoretical models and previous studies of broadband
adoption and used them as a basis for their variable choice. This study also directly specified the
potential theoretical contribution: “Findings that emerge from the data can be compared with prior theory and
any unusual findings can suggest opportunities for theory extension or modification” (p.453). A second link to
37
theory is at the construct operationalization stage. In studies that are aimed at generating new theory,
the choice of variables should of course be motivated by and related to previous studies and existing
models. However, if the  is to assess the predictability of a phenomenon or to establish a
benchmark of potential predictive accuracy, then construct considerations are negligible. Finally,
research conclusions should specifically show how the empirical results contribute to the theoretical
body of knowledge. As mentioned earlier, the contribution can be in terms of one or more of the six
roles: discovering new relationships potentially leading to new theory, contributing to measure
development, improving existing theoretical models, comparing existing theories, establishing the
relevance of existing models, and assessing predictability of empirical phenomena.
In light of our IS literature survey, a question that arises is whether the under-representation of
predictive analytics in mainstream IS literature indicates that such research is not being conducted
within the field of IS, or that such research exists but does not get published in these top two
s. A related question is why most published explanatory statistical models lack predictive
testing. We do not aim to answer these questions, although we suspect that the situation is partly
due to the traditional conflation of explanatory power with predictive accuracy. Classic statistical
education and textbooks focus on explanatory statistical modeling and statistical inference, and very
rarely discuss prediction other than in the context of prediction intervals for linear regression.
Predictive analytics are taught in machine learning, data mining and related fields. Thus, the
unfamiliarity of most IS researchers with predictive analytics may be another reason why we see little
of it so far in the IS field. We hope that this research essay convinces IS researchers to employ more
predictive analytics, but not only when the main  is predictive. Even when the main  of the
modeling is explanatory, augmenting the modeling with predictive power evaluation is easily done
and can add substantial insight. We therefore strongly advocate adopting and reporting predictive
38
power as accepted practice in empirical IS literature. We predict that increased application of
predictive analytics in the IS field holds great theoretical and practical value.
Acknowledgements
The authors thank the senior editor, three anonymous referees, and many colleagues for
constructive comments and suggestions that improved this essay. We also thank Raquelle Azran for
meticulous editorial assistance.
39
REFERENCES
Afshartous D and de Leeuw J. 2005. Prediction in Multilevel Models.  of Educational and
Behavioral Statistics. 30(2): 109-139.
Ariely D and Simonson I. 2003. Buying, bidding, playing, or competing? Value assessment and
decision dynamics in  auctions.  of Consumer Psychology 13(1-2): 113-123.
Armstrong JS. 2001. Principles of Forecasting – A Handbook for Researchers and Practitioners, Springer.
Bapna R, Jank W and Shmueli G. 2008. Price Formation and its Dynamics in  Auctions.
Decision Support Systems, 44: 641-656.
Bajari P and Hortacsu A. 2004. Economic Insights from Internet Auctions,  of Economic
Literature, 42(2): 457-486.
Barrett JP. 1974. The Coefficient of Determination - Some Limitations. The American Statistician,
28(1):19-20.
Berk RA. 2008. Statistical Learning from a Regression Perspective. Springer.
Breiman L. 1996. Bagging Predictors. Machine Learning, 24:123-140.
Breiman L. 2001a. Statistical Modeling: The Two Cultures. Statistical Science 16:199-215
Breiman L. 2001b. Random Forests. Machine Learning . 45:5-32.
Compeau DR, Meister DB and Higgins CA. 2007. From Prediction to Explanation:
Reconceptualizing and Extending the Perceived Characteristics of Innovation,  of the
Association of Information Systems, 8(8):409-439.
Copas JB. 1983. Regression, Prediction and Shrinkage.  of the Royal Statistical Society B. 45:311-
354.
40
Collopy F, Adya M, and Armstrong JS. 1994. Principles for Examining Predictive-Validity - The
Case of Information-Systems Spending Forecasts. IS Research 5(2):170-179.
Davis FD. 1989. Perceived Usefulness, Perceived Ease of Use, and User Acceptance of Information
Technology. MIS Quarterly. 13(3):319-340.
Dawes RM. 1979. The robust beauty of improper linear models in decision making. American
Psychologist, 34(7): 571-582.
Dellarocas C, Awad NF, and Zhang X. 2007. Exploring the Value of  Product Ratings in
Revenue Forecasting: The Case of Motion Pictures.  of Interactive Marketing, 21(4):23-45.
Ding Y and Simonoff JS. 2010. An Investigation of Missing Data Methods for Classification Trees
Applied to Binary Response Data.  of Machine Learning Research 11:131-170.
Dowe DL, Gardner S and Oppy GR. 2007. Bayes not Bust! Why Simplicity is No Problem for
Bayesians. British  for the Philosophy of Science, 58(4):709-754.
Dubin R. 1969. Theory building. New York: The Free Press.
Ehrenberg ASC and Bound JA. 1993. Predictability and Prediction.  of the Royal Statistical Society
Series A, 156(2): 167-206
Fayyad UM, Grinstein GG, and Wierse A. 2002. Information Visualization in Data Mining and Knowledge
Discovery. Morgan Kaufmann.
Forster MR and Sober E. 1994. How To Tell When Simpler, More Unified, or Less Ad-Hoc
Theories Will Provide More Accurate Predictions. British  for the Philosophy of Science
45(1):1-35
Forster MR. 2002. Predictive Accuracy as an Achievable  of Science. Philosophy of Science 69(3):
S124-S134
41
Friedman DJ and Montgomery DC. 1985. Evaluation of the Predictive Performance of Biased
Regression Estimators.  of Forecasting, 4:153-163.
Friedman JH. 1997. On Bias, Variance, 0/1-Loss, and the Curse-of-Dimensionality. Data Mining and
Knowledge Discovery, 1: 55-77.
Friedman JH. 2006. Comment: Classifier Technology and the Illusion of Progress, Statistical Science.
21(1): 15-18
Gefen D, Karahanna E, and Straub DW. 2003. Trust and TAM in  Shopping: An Integrated
Model. MIS Quarterly 27(1):51-90
Geman S, Bienenstock E, and Doursat R. 1992. Neural Networks and The Bias/Variance Dilemma.
Neural Computation 4:1-58.
Gifford DK. 2001. Blazing Pathways Through Genetic Mountains. Science. 293(5537):2049-2051.
Glaser BG and Strauss AL. 1980. The Discovery of Grounded Theory: Strategies for Qualitative Research. 11th
edition. New York: Aldine Publishing Company.
Grandon EE and Pearson JM. 2004. Electronic Commerce Adoption: An Empirical Study of Small
and Medium US Businesses. Information & Management. 42(1):197-216.
Gregor S. 2006. The Nature of Theory in IS. MIS Quarterly. 30(3):611-642.
Gurbaxani V and Mendelson H. 1994. Modeling vs. Forecasting: The Case of Information Systems
Spending. Information Systems Research. 5(2):180-190.
Hand DJ. 2006. Classifier Technology and the Illusion of Progress. Statistical Science. 21(1):1-14.
Hastie T, Tibshirani R, and Friedman JH. 2008. The Elements of Statistical Learning: Data Mining,
Inference, and Prediction, Springer, 2nd edition.
42
Hauser JR, Urban GL, Liberali G and Braun M. 2009. Website Morphing. Marketing Science.
28(2):202-223.
Hitchcock C and Sober E. 2004. Prediction Versus Accommodation and the Risk of Overfitting.
British  for the Philosophy of Science. 55(1):1-34.
Hoeffler S. 2002. Measuring Preferences for Really New Products.  of Marketing Research.
40(4):406-21.
Jank W and Shmueli G. 2010. Modeling  Auctions, John Wiley & Sons, New Jersey.
Joshi AJ. 1991. Natural Language Processing. Science. 523(5025):1242-1249.
Kaplan A. 1964 The Conduct of Inquiry: Methodology for Behavioral Science. Chandler Publishing, New
York.
Keil M, Mann J and Rai A. 2000. Why Software Projects Escalate: An Empirical Analysis and Test
of Four Theoretical Models, MIS Quarterly. 24(4): 631-664.
Ko M and Osei-Bryson K-M. 2008. Reexamining the Impact of Information Technology
Investment on Productivity using Regression Tree and Multivariate Adaptive Regression
Splines (MARS). Information Technology and Management. 9(4):285-299.
Konishi S and Kitagawa G. 2007. Information Criteria and Statistical Modeling. Springer.
Li X and Hitt LM. 2008. Self-Selection and Information Role of  Product Reviews. Information
Systems Research 19(4): 456-474.
Lin M., Lucas HJ and Shmueli G. 2008. Is more Always Better? Larger Samples and False
Discoveries. Working paper RHS 06-068, Robert H Smith School of Business,  of Maryland.
Little RJA and Rubin DB. 2002. Statistical Analysis with Missing Data, Wiley New York, 2nd edition.
43
Makridakis S, Hogarth RM, and Gaba A. 2009. Forecasting and Uncertainty in the Economic and
Business World, International  on Forecasting. 25(4):794-812.
Makridakis S and Taleb N. 2009. Decision Making and Planning Under Low Levels of Predictability,
International  on Forecasting. 25(4): 716-733.
Malhotra NK, Kim SS and Agarwal J. 2004. Internet Users' Information Privacy Concerns (IUIPC):
The Construct, the Scale, and a Causal Model. Information Systems Research. 15(4): 336-355.
Marcolin BL, Compeau DR, Munro MC and Huff SL. 2000. Assessing User Competence:
Conceptualization and Measurement. Information Systems Research. 11(1):37-60.
Marcoulides GA and Saunders C. 2006. PLS: A silver bullet? MIS Quarterly. 30(2): III-IV.
Mendenhall W and Sinich T. 1989. A Second Course in Business Statistics: Regression Analysis. Dellen
Publishing Company, 3rd edition.
Montgomery DC, Myers RH, Carter WH, and Vining GG. 2005. The Hierarchy Principle in
Designed Industrial Experiments. Quality and Reliability Engineering International. 21:197-201.
Mosteller F and Tukey JW. 1977. Data Analysis and Regression. Reading, Mass.: Addison-Wesley
Padmanabhan B, Zheng Z and Kimbrough SO. 2006. An Empirical Analysis of The Value of
Complete Information for eCRM Models. MIS Quarterly. 30(2):247-267.
Petter S, Straub D and Rai A. 2007. Specifying Formative Constructs in Information Systems
Research. MIS Quarterly. 31(4):623-656.
Pinker E, Seidmann A and Vakrat Y. 2003. The Design of  Auctions: Business Issues and
Current Research. Management Science. 49(11):1457-1484.
Rosenbaum PR. 2002. Observational Studies. Springer, 2nd edition.
44
Saar-Tsechansky M and Provost F. 2007. Handling Missing Features when Applying Classification
Models.  of Machine Learning Research. 8(July):1625-1657.
Sethi V and King RC. 1999. Nar and Noncompensatory Models in User Information
Satisfaction Measurement. Information Systems Research. 10(1):87-96.
Shapire RE. 1999. A Brief Introduction to Boosting. In Proceedings of the Sixth International Joint
Conference on Artificial Intelligence.
Shmueli G. 2010. To Explain or To Predict? Statistical Science, In Press.
Shmueli G, Patel NR, and Bruce PC. 2010. Data Mining for Business Intelligence: Concepts, Techniques, and
Applications in Microsoft Office Excel with XLMiner, John Wiley & Sons, 2nd edition.
Sober E. 2002. Instrumentalism, Parsimony, and the Akaike Framework. Philosophy of Science.
69(3):S112-S123.
Sober E. 2006. The Philosophy of Science: An Encyclopedia, entry “Parsimony”. Eds: Sarkar S and Pfeifer
J, Routledge, pp. 531-538.
Stern SE, Gregor S, Martin MA, Goode S, and Rolfe J. 2004. A Classification Tree Analysis of
Broadband Adoption in Australian Households. Proceedings of the 6th international conference on
Electronic commerce: pp. 451-456.
Stone M. 1974. Cross-validatory Choice and Assessment of Statistical Predictions (with Discussion).
 of the Royal Statistical Society B. 36: 111-147.
Szajna B. 1994. Software Evaluation and Choice: Predictive Validation of the Technology
Acceptance Instrument. MIS Quarterly. 18(3): 319-324.
Taleb N. 2007. The Black Swan. Penguin Books.
45
Van Maanen J, Sorensen JB and Mitchell TR. 2007. The Interplay Between Theory and Method.
Academy of Management Review. 32(4):1145-1154.
Vaughn TS and Berry KE. 2005. Using Monte Carlo Techniques to Demonstrate the Meaning and
Implications of Multicollinearity.  of Statistics Education. 13(1).
Wang S, Jank W, and Shmueli G. 2008a. Explaining and Forecasting  Auction Prices and
Their Dynamics Using Functional Data Analysis.  of Business and Economic Statistics.
26(3):144-160.
Wang T-W, Rees J and Kannan, KN. 2008b. The Association between the Disclosure and the
Realization of Information Security Risk. Working Paper, Purdue  (Available at SSRN:
http://ssrn.com/abstract=1083992)
Wu J, Cook VJ Jr. and Strong EC. 2005. A Two-Stage Model of the Promotional Performance of
Pure  Firms. Information Systems Research. 16(4):334-351.
Wu S Harris TJ and McAuley KB. 2007. The Use of Simplified and Misspecified Models: Linear
Case, Canadian  of Chemical Engineering. 85:386-398.
46
Appendix: Table 4: Illustrative quotes from the literature review
Article
 Quote
Rai, A., Patnayakuni, R., and Seth, N. “Firm
 “One indicator of the predictive power of path models is to
performance impacts of digitally enabled supply
 examine the explained variance or R2 values” (p.235)
chain integration capabilities,” MIS Quarterly
30(2), Jun 2006, pp 225-246.
Pavlou, P.A., and Fygenson, M. “Understanding
 “To examine the predictive power of the proposed model, we
and predicting electronic commerce adoption:
 compare it to four models in terms of R 2 adjusted” (p.131)
An extension of the theory of planned
behavior,” MIS Quarterly 30(1), Mar 2006, pp
115-143.
Gattiker, T.F., and Goodhue, D.L. “What
 “However, coordination benefits do not predict overall ERP
happens
 after
 ERP
 implementation:
 benefits as strongly as do task efficiency and data quality
Understanding the impact of interdependence
 (as the standardized regression coefficients in Figure 2
and differentiation on plant-level outcomes,”
 indicate)” (p.579)
MIS Quarterly 29(3), Sep 2005, pp 559-585.
Venkatesh, V., Morris, M.G., Davis, G.B., and
 “With the exception of MM and SCT, the predictive
Davis, F.D. "User acceptance of information
 validity of the models increased after including the
technology: Toward a unified view,” MIS
 moderating variables. For instance, the variance explained
Quarterly 27(3), Sep 2003, pp 425-478.
 by TAM2 increased to 53 percent.” (p.445)
Wixom, B.H., and Todd, P.A. “A theoretical
 “Usefulness and attitude again dominate in the prediction
integration of user satisfaction and technology
 of intention, and the remaining path coefficients are
acceptance,” Information Systems Research 16(1),
 generally small (8 of 13 are below 0.1). The explanatory
Mar 2005, pp 85-102.
 power for intention increases marginally from 0.59 to
0.63.” (p.97)
Jones, Q., Ravid, G., and Rafaeli, S. “Unfortunately, while the ranking and variable matching
“Information overload and the message enabled regression modeling, this approach results in a loss
dynamics of  interaction spaces: A of variance and predictive/explanatory power.” (p.203)
theoretical model and empirical exploration,”
Information Systems Research 15(2), Jun 2004, pp
194-210.
Jarvenpaa, S.L., Shaw, T.R., and Staples, D.S. “The predictive power of the model (i.e., variance explained)
“Toward contextualized theories of trust: The was quite high in Study 1” (p.262)
role of trust in global virtual teams,” Information
Systems Research 15(3), Sep 2004, pp 250-267.
Bassellier, G., Benbasat, I., and Reich, B.H. “The “We can also assess the completeness of our constructs by
influence of business managers' IT competence examining their ability to predict the measured overall IT
on championing IT,” Information Systems Research knowledge and IT experience. The second order factor IT
14(4), Dec 2003, pp 317-336.
 knowledge explains 71% of the variance in the overall IT
knowledge.” (p.331)
Kraut, R, Mukhopadhyay, T, Szczypula, J,
 “One can predict a participant's current e-mail use from his
Kiesler, S, and Scherlis, B. “Information and
 or her use in the prior week much better than one can
Communication: Alternative Uses of the Internet
 predict a participant's current Web use from his or her prior
in Households.Preview”, Information Systems


Predictive analytics encompasses a variety of statistical techniques from predictive modeling, machine learning, and data mining that analyze current and historical facts to make predictions about future or otherwise unknown events.[1][2][3]

In business, predictive models exploit patterns found in historical and transactional data to identify risks and opportunities. Models capture relationships among many factors to allow assessment of risk or potential associated with a particular set of conditions, guiding decision making for candidate transactions.[4]

The defining functional effect of these technical approaches is that predictive analytics provides a predictive score (probability) for each individual (customer, employee, healthcare patient, product SKU, vehicle, component, machine, or other organizational unit) in order to determine, inform, or influence organizational processes that pertain across large numbers of individuals, such as in marketing, credit risk assessment, fraud detection, manufacturing, healthcare, and government operations including law enforcement.

Predictive analytics is used in actuarial science,[5] marketing,[1][6] financial services,[7] insurance, telecommunications,[8] retail,[9] travel,[10] healthcare,[11] child protection,[12][13] pharmaceuticals,[14] capacity planning[citation needed] and other fields.

One of the most well known applications is credit scoring,[2] which is used throughout financial services. Scoring models process a customer's credit history, loan application, customer data, etc., in order to rank-order individuals by their likelihood of making future credit payments on time.

Contents

    1 Definition
    2 Types
        2.1 Predictive models
        2.2 Descriptive models
        2.3 Decision models
    3 Applications
        3.1 Analytical customer relationship management (CRM)
        3.2 Child protection
        3.3 Clinical decision support systems
        3.4 Collection analytics
        3.5 Cross-sell
        3.6 Customer retention
        3.7 Direct marketing
        3.8 Fraud detection
        3.9 Portfolio, product or economy-level prediction
        3.10 Risk management
        3.11 Underwriting
    4 Technology and big data influences
    5 Analytical Techniques
        5.1 Regression techniques
            5.1.1 Linear regression model
            5.1.2 Discrete choice models
            5.1.3 Logistic regression
            5.1.4 Multinomial logistic regression
            5.1.5 Probit regression
            5.1.6 Logit versus probit
            5.1.7 Time series models
            5.1.8 Survival or duration analysis
            5.1.9 Classification and regression trees (CART)
            5.1.10 Multivariate adaptive regression splines
        5.2 Machine learning techniques
            5.2.1 Neural networks
            5.2.2 Multilayer Perceptron (MLP)
            5.2.3 Radial basis functions
            5.2.4 Support vector machines
            5.2.5 Naïve Bayes
            5.2.6 k-nearest neighbours
            5.2.7 Geospatial predictive modeling
    6 Tools
        6.1 PMML
    7 Criticism
    8 See also
    9 References
    10 Further reading

Definition

Predictive analytics is an area of data mining that deals with extracting information from data and using it to predict trends and behavior patterns.[1] Often the unknown event of interest is in the future, but predictive analytics can be applied to any type of unknown whether it be in the past, present or future. For example, identifying suspects after a crime has been committed, or credit card fraud as it occurs.[15] The core of predictive analytics relies on capturing relationships between explanatory variables and the predicted variables from past occurrences, and exploiting them to predict the unknown outcome. It is important to note, however, that the accuracy and usability of results will depend greatly on the level of data analysis and the quality of assumptions.

Predictive analytics is often defined as predicting at a more detailed level of granularity, i.e., generating predictive scores (probabilities) for each individual organizational element. This distinguishes it from forecasting. For example, "Predictive analytics—Technology that learns from experience (data) to predict the future behavior of individuals in order to drive better decisions."[16] In the future industrial systems the value of Predictive Analytics is to predict and prevent potential issues to achieve near-zero break-down and further be integrated into prescriptive analytics for decision optimization. Furthermore, the converted data can be used for closed-loop product life cycle improvement[17] which is the vision of Industrial Internet Consortium.
Types

Generally, the term predictive analytics is used to mean predictive modeling, "scoring" data with predictive models, and forecasting. However, people are increasingly using the term to refer to related analytical disciplines, such as descriptive modeling and decision modeling or optimization. These disciplines also involve rigorous data analysis, and are widely used in business for segmentation and decision making, but have different purposes and the statistical techniques underlying them vary.
Predictive models

Predictive models are models of the relation between the specific performance of a unit in a sample and one or more known attributes or features of the unit. The objective of the model is to assess the likelihood that a similar unit in a different sample will exhibit the specific performance. This category encompasses models in many areas, such as marketing, where they seek out subtle data patterns to answer questions about customer performance, or fraud detection models. Predictive models often perform calculations during live transactions, for example, to evaluate the risk or opportunity of a given customer or transaction, in order to guide a decision. With advancements in computing speed, individual agent modeling systems have become capable of simulating human behaviour or reactions to given stimuli or scenarios.

The available sample units with known attributes and known performances is referred to as the “training sample.” The units in other samples, with known attributes but unknown performances, are referred to as “out of [training] sample” units. The out of sample bear no chronological relation to the training sample units. For example, the training sample may consists of literary attributes of writings by Victorian authors, with known attribution, and the out-of sample unit may be newly found writing with unknown authorship; a predictive model may aid in attributing a work to a known author. Another example is given by analysis of blood splatter in simulated crime scenes in which the out of sample unit is the actual blood splatter pattern from a crime scene. The out of sample unit may be from the same time as the training units, from a previous time, or from a future time.
Descriptive models

Descriptive models quantify relationships in data in a way that is often used to classify customers or prospects into groups. Unlike predictive models that focus on predicting a single customer behavior (such as credit risk), descriptive models identify many different relationships between customers or products. Descriptive models do not rank-order customers by their likelihood of taking a particular action the way predictive models do. Instead, descriptive models can be used, for example, to categorize customers by their product preferences and life stage. Descriptive modeling tools can be utilized to develop further models that can simulate large number of individualized agents and make predictions.
Decision models

Decision models describe the relationship between all the elements of a decision — the known data (including results of predictive models), the decision, and the forecast results of the decision — in order to predict the results of decisions involving many variables. These models can be used in optimization, maximizing certain outcomes while minimizing others. Decision models are generally used to develop decision logic or a set of business rules that will produce the desired action for every customer or circumstance.
Applications

Although predictive analytics can be put to use in many applications, we outline a few examples where predictive analytics has shown positive impact in recent years.
Analytical customer relationship management (CRM)

Analytical Customer Relationship Management is a frequent commercial application of Predictive Analysis. Methods of predictive analysis are applied to customer data to pursue CRM objectives, which involve constructing a holistic view of the customer no matter where their information resides in the company or the department involved. CRM uses predictive analysis in applications for marketing campaigns, sales, and customer services to name a few. These tools are required in order for a company to posture and focus their efforts effectively across the breadth of their customer base. They must analyze and understand the products in demand or have the potential for high demand, predict customers' buying habits in order to promote relevant products at multiple touch points, and proactively identify and mitigate issues that have the potential to lose customers or reduce their ability to gain new ones. Analytical Customer Relationship Management can be applied throughout the customers lifecycle (acquisition, relationship growth, retention, and win-back). Several of the application areas described below (direct marketing, cross-sell, customer retention) are part of Customer Relationship Managements.
Child protection

Over the last 5 years, some Child Welfare Agencies have started using predictive analytics to flag high risk cases.[18] The approach has been called "innovative" by the Commission to Eliminate Child Abuse and Neglect Fatalities (CECANF),[19] and in Hillsborough County, FL, where the Lead Child Welfare Agency uses a predictive modeling tool called Eckerd Rapid Safety Feedback®, there have been no abuse-related child deaths in the target population as of this writing.[20]
Clinical decision support systems

Experts use predictive analysis in health care primarily to determine which patients are at risk of developing certain conditions, like diabetes, asthma, heart disease, and other lifetime illnesses. Additionally, sophisticated clinical decision support systems incorporate predictive analytics to support medical decision making at the point of care. A working definition has been proposed by Robert Hayward of the Centre for Health Evidence: "Clinical Decision Support Systems link health observations with health knowledge to influence health choices by clinicians for improved health care."[citation needed]
Collection analytics

Many portfolios have a set of delinquent customers who do not make their payments on time. The financial institution has to undertake collection activities on these customers to recover the amounts due. A lot of collection resources are wasted on customers who are difficult or impossible to recover. Predictive analytics can help optimize the allocation of collection resources by identifying the most effective collection agencies, contact strategies, legal actions and other strategies to each customer, thus significantly increasing recovery at the same time reducing collection costs.
Cross-sell

Often corporate organizations collect and maintain abundant data (e.g. customer records, sale transactions) as exploiting hidden relationships in the data can provide a competitive advantage. For an organization that offers multiple products, predictive analytics can help analyze customers' spending, usage and other behavior, leading to efficient cross sales, or selling additional products to current customers.[3] This directly leads to higher profitability per customer and stronger customer relationships.
Customer retention

With the number of competing services available, businesses need to focus efforts on maintaining continuous consumer satisfaction, rewarding consumer loyalty and minimizing customer attrition. In addition, small increases in customer retention have been shown to increase profits disproportionately. One study concluded that a 5% increase in customer retention rates will increase profits by 25% to 95%.[21] Businesses tend to respond to customer attrition on a reactive basis, acting only after the customer has initiated the process to terminate service. At this stage, the chance of changing the customer's decision is almost zero. Proper application of predictive analytics can lead to a more proactive retention strategy. By a frequent examination of a customer’s past service usage, service performance, spending and other behavior patterns, predictive models can determine the likelihood of a customer terminating service sometime soon.[8] An intervention with lucrative offers can increase the chance of retaining the customer. Silent attrition, the behavior of a customer to slowly but steadily reduce usage, is another problem that many companies face. Predictive analytics can also predict this behavior, so that the company can take proper actions to increase customer activity.
Direct marketing

When marketing consumer products and services, there is the challenge of keeping up with competing products and consumer behavior. Apart from identifying prospects, predictive analytics can also help to identify the most effective combination of product versions, marketing material, communication channels and timing that should be used to target a given consumer. The  of predictive analytics is typically to lower the cost per order or cost per action.
Fraud detection

Fraud is a big problem for many businesses and can be of various types: inaccurate credit applications, fraudulent transactions (both offline and ), identity thefts and false insurance claims. These problems plague firms of all sizes in many industries. Some examples of likely victims are credit card issuers, insurance companies,[22] retail merchants, manufacturers, business-to-business suppliers and even services providers. A predictive model can help weed out the "bads" and reduce a business's exposure to fraud.

Predictive modeling can also be used to identify high-risk fraud candidates in business or the public sector. Mark Nigrini developed a risk-scoring method to identify audit targets. He describes the use of this approach to detect fraud in the franchisee sales reports of an international fast-food chain. Each location is scored using 10 predictors. The 10 scores are then weighted to give one final overall risk score for each location. The same scoring approach was also used to identify high-risk check kiting accounts, potentially fraudulent travel agents, and questionable vendors. A reasonably complex model was used to identify fraudulent monthly reports submitted by divisional controllers.[23]

The Internal Revenue Service (IRS) of the United States also uses predictive analytics to mine tax returns and identify tax fraud.[22]

Recent[when?] advancements in technology have also introduced predictive behavior analysis for web fraud detection. This type of solution utilizes heuristics in order to study normal web user behavior and detect anomalies indicating fraud attempts.
Portfolio, product or economy-level prediction

Often the focus of analysis is not the consumer but the product, portfolio, firm, industry or even the economy. For example, a retailer might be interested in predicting store-level demand for inventory management purposes. Or the Federal Reserve Board might be interested in predicting the unemployment rate for the next year. These types of problems can be addressed by predictive analytics using time series techniques (see below). They can also be addressed via machine learning approaches which transform the original time series into a feature vector space, where the learning algorithm finds patterns that have predictive power.[24][25]
Risk management

When employing risk management techniques, the results are always to predict and benefit from a future scenario. The Capital asset pricing model (CAP-M) "predicts" the best portfolio to maximize return, Probabilistic Risk Assessment (PRA)--when combined with mini-Delphi Techniques and statistical approaches yields accurate forecasts and RiskAoA is a stand-alone predictive tool.[26] These are three examples of approaches that can extend from project to market, and from near to long term. Underwriting (see below) and other business approaches identify risk management as a predictive method.
Underwriting

Many businesses have to account for risk exposure due to their different services and determine the cost needed to cover the risk. For example, auto insurance providers need to accurately determine the amount of premium to charge to cover each automobile and driver. A financial company needs to assess a borrower's potential and ability to pay before granting a loan. For a health insurance provider, predictive analytics can analyze a few years of past medical claims data, as well as lab, pharmacy and other records where available, to predict how expensive an enrollee is likely to be in the future. Predictive analytics can help underwrite these quantities by predicting the chances of illness, default, bankruptcy, etc. Predictive analytics can streamline the process of customer acquisition by predicting the future risk behavior of a customer using application level data.[5] Predictive analytics in the form of credit scores have reduced the amount of time it takes for loan approvals, especially in the mortgage market where lending decisions are now made in a matter of hours rather than days or even weeks. Proper predictive analytics can lead to proper pricing decisions, which can help mitigate future risk of default.
Technology and big data influences

Big data is a collection of data sets that are so large and complex that they become awkward to work with using traditional database management tools. The volume, variety and velocity of big data have introduced challenges across the board for capture, storage, search, sharing, analysis, and visualization. Examples of big data sources include web logs, RFID, sensor data, social networks, Internet search indexing, call detail records, military surveillance, and complex data in astronomic, biogeochemical, genomics, and atmospheric sciences. Big Data is the core of most predictive analytic services offered by IT organizations.[27] Thanks to technological advances in computer hardware — faster CPUs, cheaper memory, and MPP architectures — and new technologies such as Hadoop, MapReduce, and in-database and text analytics for processing big data, it is now feasible to collect, analyze, and mine massive amounts of structured and unstructured data for new insights.[22] It is also possible to run predictive algorithms on streaming data.[28] Today, exploring big data and using predictive analytics is within reach of more organizations than ever before and new methods that are capable for handling such datasets are proposed [29] [30]
Analytical Techniques

The approaches and techniques used to conduct predictive analytics can broadly be grouped into regression techniques and machine learning techniques.
Regression techniques

Regression models are the mainstay of predictive analytics. The focus lies on establishing a mathematical equation as a model to represent the interactions between the different variables in consideration. Depending on the situation, there are a wide variety of models that can be applied while performing predictive analytics. Some of them are briefly discussed below.
Linear regression model

The linear regression model analyzes the relationship between the response or dependent variable and a set of independent or predictor variables. This relationship is expressed as an equation that predicts the response variable as a linear function of the parameters. These parameters are adjusted so that a measure of fit is optimized. Much of the effort in model fitting is focused on minimizing the size of the residual, as well as ensuring that it is randomly distributed with respect to the model predictions.

The  of regression is to select the parameters of the model so as to minimize the sum of the squared residuals. This is referred to as ordinary least squares (OLS) estimation and results in best linear unbiased estimates (BLUE) of the parameters if and only if the Gauss-Markov assumptions are satisfied.

Once the model has been estimated we would be interested to know if the predictor variables belong in the model – i.e. is the estimate of each variable's contribution reliable? To do this we can check the statistical significance of the model’s coefficients which can be measured using the t-statistic. This amounts to testing whether the coefficient is significantly different from zero. How well the model predicts the dependent variable based on the value of the independent variables can be assessed by using the R² statistic. It measures predictive power of the model i.e. the proportion of the total variation in the dependent variable that is "explained" (accounted for) by variation in the independent variables.
Discrete choice models

Multivariate regression (above) is generally used when the response variable is continuous and has an unbounded range. Often the response variable may not be continuous but rather discrete. While mathematically it is feasible to apply multivariate regression to discrete ordered dependent variables, some of the assumptions behind the theory of multivariate linear regression no longer hold, and there are other techniques such as discrete choice models which are better suited for this type of analysis. If the dependent variable is discrete, some of those superior methods are logistic regression, multinomial logit and probit models. Logistic regression and probit models are used when the dependent variable is binary.
Logistic regression
For more details on this topic, see logistic regression.

In a classification setting, assigning outcome probabilities to observations can be achieved through the use of a logistic model, which is basically a method which transforms information about the binary dependent variable into an unbounded continuous variable and estimates a regular multivariate model (See Allison's Logistic Regression for more information on the theory of Logistic Regression).

The Wald and likelihood-ratio test are used to test the statistical significance of each coefficient b in the model (analogous to the t tests used in OLS regression; see above). A test assessing the goodness-of-fit of a classification model is the "percentage correctly predicted".
Multinomial logistic regression

An extension of the binary logit model to cases where the dependent variable has more than 2 categories is the multinomial logit model. In such cases collapsing the data into two categories might not make good sense or may lead to loss in the richness of the data. The multinomial logit model is the appropriate technique in these cases, especially when the dependent variable categories are not ordered (for examples colors like red, blue, green). Some authors have extended multinomial regression to include feature selection/importance methods such as Random multinomial logit.
Probit regression

Probit models offer an alternative to logistic regression for modeling categorical dependent variables. Even though the outcomes tend to be similar, the underlying distributions are different. Probit models are popular in social sciences like economics.

A good way to understand the key difference between probit and logit models is to assume that the dependent variable is driven by a latent variable z, which is a sum of a linear combination of explanatory variables and a random noise term.

We do not observe z but instead observe y which takes the value 0 (when z < 0) or 1 (otherwise). In the logit model we assume that the random noise term follows a logistic distribution with mean zero. In the probit model we assume that it follows a normal distribution with mean zero. Note that in social sciences (e.g. economics), probit is often used to model situations where the observed variable y is continuous but takes values between 0 and 1.
Logit versus probit

The Probit model has been around longer than the logit model. They behave similarly, except that the logistic distribution tends to be slightly flatter tailed. One of the reasons the logit model was formulated was that the probit model was computationally difficult due to the requirement of numerically calculating integrals. Modern computing however has made this computation fairly simple. The coefficients obtained from the logit and probit model are fairly close. However, the odds ratio is easier to interpret in the logit model.

Practical reasons for choosing the probit model over the logistic model would be:

    There is a strong belief that the underlying distribution is normal
    The actual event is not a binary outcome (e.g., bankruptcy status) but a proportion (e.g., proportion of population at different debt levels).

Time series models

Time series models are used for predicting or forecasting the future behavior of variables. These models account for the fact that data points taken over time may have an internal structure (such as autocorrelation, trend or seasonal variation) that should be accounted for. As a result, standard regression techniques cannot be applied to time series data and methodology has been developed to decompose the trend, seasonal and cyclical component of the series. Modeling the dynamic path of a variable can improve forecasts since the predictable component of the series can be projected into the future.

Time series models estimate difference equations containing stochastic components. Two commonly used forms of these models are autoregressive models (AR) and moving-average (MA) models. The Box–Jenkins methodology (1976) developed by George Box and G.M. Jenkins combines the AR and MA models to produce the ARMA (autoregressive moving average) model which is the cornerstone of stationary time series analysis. ARIMA (autoregressive integrated moving average models) on the other hand are used to describe non-stationary time series. Box and Jenkins suggest differencing a non stationary time series to obtain a stationary series to which an ARMA model can be applied. Non stationary time series have a pronounced trend and do not have a constant long-run mean or variance.

Box and Jenkins proposed a three-stage methodology which includes: model identification, estimation and validation. The identification stage involves identifying if the series is stationary or not and the presence of seasonality by examining plots of the series, autocorrelation and partial autocorrelation functions. In the estimation stage, models are estimated using non-linear time series or maximum likelihood estimation procedures. Finally the validation stage involves diagnostic checking such as plotting the residuals to detect outliers and evidence of model fit.

In recent years time series models have become more sophisticated and attempt to model conditional heteroskedasticity with models such as ARCH (autoregressive conditional heteroskedasticity) and GARCH (generalized autoregressive conditional heteroskedasticity) models frequently used for financial time series. In addition time series models are also used to understand inter-relationships among economic variables represented by systems of equations using VAR (vector autoregression) and structural VAR models.
Survival or duration analysis

Survival analysis is another name for time to event analysis. These techniques were primarily developed in the medical and biological sciences, but they are also widely used in the social sciences like economics, as well as in engineering (reliability and failure time analysis).

Censoring and non-normality, which are characteristic of survival data, generate difficulty when trying to analyze the data using conventional statistical models such as multiple linear regression. The normal distribution, being a symmetric distribution, takes positive as well as negative values, but duration by its very nature cannot be negative and therefore normality cannot be assumed when dealing with duration/survival data. Hence the normality assumption of regression models is violated.

The assumption is that if the data were not censored it would be representative of the population of interest. In survival analysis, censored observations arise whenever the dependent variable of interest represents the time to a terminal event, and the duration of the study is limited in time.

An important concept in survival analysis is the hazard rate, defined as the probability that the event will occur at time t conditional on surviving until time t. Another concept related to the hazard rate is the survival function which can be defined as the probability of surviving to time t.

Most models try to model the hazard rate by choosing the underlying distribution depending on the shape of the hazard function. A distribution whose hazard function slopes upward is said to have positive duration dependence, a decreasing hazard shows negative duration dependence whereas constant hazard is a process with no memory usually characterized by the exponential distribution. Some of the distributional choices in survival models are: F, gamma, Weibull, log normal, inverse normal, exponential etc. All these distributions are for a non-negative random variable.

Duration models can be parametric, non-parametric or semi-parametric. Some of the models commonly used are Kaplan-Meier and Cox proportional hazard model (non parametric).
Classification and regression trees (CART)
Main article: decision tree learning

Globally-optimal classification tree analysis (GO-CTA) (also called hierarchical optimal discriminant analysis) is a generalization of optimal discriminant analysis that may be used to identify the statistical model that has maximum accuracy for predicting the value of a categorical dependent variable for a dataset consisting of categorical and continuous variables. The output of HODA is a non-orthogonal tree that combines categorical variables and cut points for continuous variables that yields maximum predictive accuracy, an assessment of the exact Type I error rate, and an evaluation of potential cross-generalizability of the statistical model. Hierarchical optimal discriminant analysis may be thought of as a generalization of Fisher's linear discriminant analysis. Optimal discriminant analysis is an alternative to ANOVA (analysis of variance) and regression analysis, which attempt to express one dependent variable as a linear combination of other features or measurements. However, ANOVA and regression analysis give a dependent variable that is a numerical variable, while hierarchical optimal discriminant analysis gives a dependent variable that is a class variable.

Classification and regression trees (CART) are a non-parametric decision tree learning technique that produces either classification or regression trees, depending on whether the dependent variable is categorical or numeric, respectively.

Decision trees are formed by a collection of rules based on variables in the modeling data set:

    Rules based on variables' values are selected to get the best split to differentiate observations based on the dependent variable
    Once a rule is selected and splits a node into two, the same process is applied to each "child" node (i.e. it is a recursive procedure)
    Splitting stops when CART detects no further gain can be made, or some pre-set stopping rules are met. (Alternatively, the data are split as much as possible and then the tree is later pruned.)

Each branch of the tree ends in a terminal node. Each observation falls into one and exactly one terminal node, and each terminal node is uniquely defined by a set of rules.

A very popular method for predictive analytics is Leo Breiman's Random forests.
Multivariate adaptive regression splines

Multivariate adaptive regression splines (MARS) is a non-parametric technique that builds flexible models by fitting piecewise linear regressions.

An important concept associated with regression splines is that of a knot. Knot is where one local regression model gives way to another and thus is the point of intersection between two splines.

In multivariate and adaptive regression splines, basis functions are the tool used for generalizing the search for knots. Basis functions are a set of functions used to represent the information contained in one or more variables. Multivariate and Adaptive Regression Splines model almost always creates the basis functions in pairs.

Multivariate and adaptive regression spline approach deliberately overfits the model and then prunes to get to the optimal model. The algorithm is computationally very intensive and in practice we are required to specify an upper limit on the number of basis functions.
Machine learning techniques

Machine learning, a branch of artificial intelligence, was originally employed to develop techniques to enable computers to learn. Today, since it includes a number of advanced statistical methods for regression and classification, it finds application in a wide variety of fields including medical diagnostics, credit card fraud detection, face and speech recognition and analysis of the stock market. In certain applications it is sufficient to directly predict the dependent variable without focusing on the underlying relationships between variables. In other cases, the underlying relationships can be very complex and the mathematical form of the dependencies unknown. For such cases, machine learning techniques emulate human cognition and learn from training examples to predict future events.

A brief discussion of some of these methods used commonly for predictive analytics is provided below. A detailed study of machine learning can be found in Mitchell (1997).
Neural networks

Neural networks are nar sophisticated modeling techniques that are able to model complex functions. They can be applied to problems of prediction, classification or control in a wide spectrum of fields such as finance, cognitive psychology/neuroscience, medicine, engineering, and physics.

Neural networks are used when the exact nature of the relationship between inputs and output is not known. A key feature of neural networks is that they learn the relationship between inputs and output through training. There are three types of training in neural networks used by different networks, supervised and unsupervised training, reinforcement learning, with supervised being the most common one.

Some examples of neural network training techniques are backpropagation, quick propagation, conjugate gradient descent, projection operator, Delta-Bar-Delta etc. Some unsupervised network architectures are multilayer perceptrons, Kohonen networks, Hopfield networks, etc.
Multilayer Perceptron (MLP)

The Multilayer Perceptron (MLP) consists of an input and an output layer with one or more hidden layers of narly-activating nodes or sigmoid nodes. This is determined by the weight vector and it is necessary to adjust the weights of the network. The backpropagation employs gradient fall to minimize the squared error between the network output values and desired values for those outputs. The weights adjusted by an iterative process of repetitive present of attributes. Small changes in the weight to get the desired values are done by the process called training the net and is done by the training set (learning rule).
Radial basis functions

A radial basis function (RBF) is a function which has built into it a distance criterion with respect to a center. Such functions can be used very efficiently for interpolation and for smoothing of data. Radial basis functions have been applied in the area of neural networks where they are used as a replacement for the sigmoidal transfer function. Such networks have 3 layers, the input layer, the hidden layer with the RBF non-linearity and a linear output layer. The most popular choice for the non-linearity is the Gaussian. RBF networks have the advantage of not being locked into local minima as do the feed-forward networks such as the multilayer perceptron.
Support vector machines

Support Vector Machines (SVM) are used to detect and exploit complex patterns in data by clustering, classifying and ranking the data. They are learning machines that are used to perform binary classifications and regression estimations. They commonly use kernel based methods to apply linear classification techniques to non-linear classification problems. There are a number of types of SVM such as linear, polynomial, sigmoid etc.
Naïve Bayes

Naïve Bayes based on Bayes conditional probability rule is used for performing classification tasks. Naïve Bayes assumes the predictors are statistically independent which makes it an effective classification tool that is easy to interpret. It is best employed when faced with the problem of ‘curse of dimensionality’ i.e. when the number of predictors is very high.
k-nearest neighbours

The nearest neighbour algorithm (KNN) belongs to the class of pattern recognition statistical methods. The method does not impose a priori any assumptions about the distribution from which the modeling sample is drawn. It involves a training set with both positive and negative values. A new sample is classified by calculating the distance to the nearest neighbouring training case. The sign of that point will determine the classification of the sample. In the k-nearest neighbour classifier, the k nearest points are considered and the sign of the majority is used to classify the sample. The performance of the kNN algorithm is influenced by three main factors: (1) the distance measure used to locate the nearest neighbours; (2) the decision rule used to derive a classification from the k-nearest neighbours; and (3) the number of neighbours used to classify the new sample. It can be proved that, unlike other methods, this method is universally asymptotically convergent, i.e.: as the size of the training set increases, if the observations are independent and identically distributed (i.i.d.), regardless of the distribution from which the sample is drawn, the predicted class will converge to the class assignment that minimizes misclassification error. See Devroy et al.
Geospatial predictive modeling

Conceptually, geospatial predictive modeling is rooted in the principle that the occurrences of events being modeled are limited in distribution. Occurrences of events are neither uniform nor random in distribution – there are spatial environment factors (infrastructure, sociocultural, topographic, etc.) that constrain and influence where the locations of events occur. Geospatial predictive modeling attempts to describe those constraints and influences by spatially correlating occurrences of historical geospatial locations with environmental factors that represent those constraints and influences. Geospatial predictive modeling is a process for analyzing events through a geographic filter in order to make statements of likelihood for event occurrence or emergence.
Tools

Historically, using predictive analytics tools—as well as understanding the results they delivered—required advanced skills. However, modern predictive analytics tools are no longer restricted to IT specialists[citation needed]. As more organizations adopt predictive analytics into decision-making processes and integrate it into their operations, they are creating a shift in the market toward business users as the primary consumers of the information. Business users want tools they can use on their own. Vendors are responding by creating new software that removes the mathematical complexity, provides user-friendly graphic interfaces and/or builds in short cuts that can, for example, recognize the kind of data available and suggest an appropriate predictive model.[31] Predictive analytics tools have become sophisticated enough to adequately present and dissect data problems[citation needed], so that any data-savvy information worker can utilize them to analyze data and retrieve meaningful, useful results.[3] For example, modern tools present findings using simple charts, graphs, and scores that indicate the likelihood of possible outcomes.[32]

There are numerous tools available in the marketplace that help with the execution of predictive analytics. These range from those that need very little user sophistication to those that are designed for the expert practitioner. The difference between these tools is often in the level of customization and heavy data lifting allowed.

Notable open source predictive analytic tools include:

    Apache Mahout
    GNU Octave
    KNIME
    OpenNN
    Orange
    R
    RiskAoA
    scikit-learn
    Weka

Notable commercial predictive analytic tools include:

    Alpine Data Labs
    Angoss KnowledgeSTUDIO
    BIRT Analytics
    IBM SPSS Statistics and IBM SPSS Modeler
    KXEN Modeler
    Mathematica
    MATLAB
    Minitab
    LabVIEW[33]
    Neural Designer
    Oracle Advanced Analytics
    Pervasive
    Predixion Software
    RapidMiner
    RCASE
    Revolution Analytics
    SAP HANA[34] and SAP BusinessObjects Predictive Analytics[35]
    SAS and SAS Enterprise Miner
    STATA
    Statgraphics
    STATISTICA
    TeleRetail
    TIBCO

Beside these software packages, specific tools have also been developed for industrial applications. For example, Watchdog Agent Toolbox has been developed and optimized for predictive analysis in prognostics and health management applications and is available for MATLAB and LABVIEW[36][37]

The most popular commercial predictive analytics software packages according to the Rexer Analytics Survey for 2013 are IBM SPSS Modeler, SAS Enterprise Miner, and Dell Statistica <http://www.rexeranalytics.com/Data-Miner-Survey-2013-Intro.html 
>.
PMML

In an attempt to provide a standard language for expressing predictive models, the Predictive Model Markup Language (PMML) has been proposed. Such an XML-based language provides a way for the different tools to define predictive models and to share these between PMML compliant applications. PMML 4.0 was released in June, 2009.
Criticism

There are plenty of skeptics when it comes to computers and algorithms abilities to predict the future, including Gary King, a professor from Harvard  and the director of the Institute for Quantitative Social Science. [38] People are influenced by their environment in innumerable ways. Trying to understand what people will do next assumes that all the influential variables can be known and measured accurately. "People's environments change even more quickly than they themselves do. Everything from the weather to their relationship with their mother can change the way people think and act. All of those variables are unpredictable. How they will impact a person is even less predictable. If put in the exact same situation tomorrow, they may make a completely different decision. This means that a statistical prediction is only valid in sterile laboratory conditions, which suddenly isn't as useful as it seemed before." [39]
See also

    Criminal Reduction Utilising Statistical History
    Data mining
    Learning analytics
    Odds algorithm
    Pattern recognition
    Prescriptive analytics
    Predictive modeling
    RiskAoA a predictive tool for discriminating future decisions.


Predictive analytics allows you to discover, analyze and act on data. It’s about learning from the past to uncover trends and predict likely outcomes. But that’s not all. Predictive analytics gives you a framework to analyze data over time, leading to more refined outcomes and corrective actions.

And using data to create a coherent view of the future has never been more important. The convergence of big data, time series data, social media, sensor data and mobile devices gives you the potential to add more fuel to your predictive analytics engine. Now it’s up to you to collect, manage and analyze this information – and position your organization for success.
Analytics Insights
Analytics Insights
Connect with the latest insights on analytics through related articles and research.
More on predictive analytics

    Drive your business with predictive analytics
    Three steps to putting predictive analytics to work

How it works

Predictive analytics combines techniques from statistics, data mining and machine learning to find meaning from large amounts of data. Whether you’re in marketing, compliance, customer service, operations or any other business unit, your data can show where you are – and predict where you’re going.

How do organizations approach predictive analytics? The key stages for an analytical life cycle include:

    Analytical data preparation – Source, clean and prepare the data for optimal results.
    Visualization and exploration – Explore all data to identify relevant variables, trends and relationships.
    Statistical analysis – Use everything from simple descriptive statistics to complex Bayesian analysis to quantify uncertainty, make inferences and drive decisions.
    Predictive modeling – Build the predictive model using statistical, data mining or text mining algorithms, including the critical capability of transforming and selecting key variables.
    Model deployment – Apply the new champion model, once validated and approved, to new data.
    Model management and monitoring – Examine model performance to make sure it is up-to-date and delivering valid results.

Predictive analytics in action

Marketing – From telecommunications to education to gaming and beyond, organizations need to forecast customer responses or purchases. Predictive models enable businesses to discover and attract the most profitable customers, helping maximize value from their marketing budget.

Risk – Credit scores assess a buyer’s likelihood to default on purchases like cars, homes or insurance. Credit scores are numbers generated by a predictive model that incorporates all data relevant to creditworthiness. Predictive analytics has other risk-related uses as well, including claims, collections, fraud and security.

Operations – To make an organization more efficient, you need to understand future needs and anticipate demand. Manufacturers need to manage inventory and factory resources. Airlines must decide how many tickets to sell at each price for a flight. Hotels try to predict the number of guests they can expect on a given night. Predictive analytics is at the heart of all of these operational decisions.


What is Predictive Analytics? Predictive analytics is the branch of the advanced analytics which is used to make predictions about unknown future events. Predictive analytics uses many techniques from data mining, statistics, modeling, machine learning, and artificial intelligence to analyze current data to make predictions about future. It uses a number of data mining, predictive modeling and analytical techniques to bring together the management, information technology, and modeling business process to make predictions about future. The patterns found in historical and transactional data can be used to identify risks and opportunities for future. Predictive analytics models capture relationships among many factors to assess risk with a particular set of conditions to assign a score, or weightage. By successfully applying predictive analytics the businesses can effectively interpret big data for their benefit.

The data mining and text analytics along with statistics, allows the business users to create predictive intelligence by uncovering patterns and relationships in both the structured and unstructured data. The data which can be used readily for analysis are structured data, examples like age, gender, marital status, income, sales. Unstructured data are textual data in call center notes, social media content, or other type of open text which need to be extracted from the text, along with the sentiment, and then used in the model building process.

Predictive analytics allows organizations to become proactive, forward looking, anticipating outcomes and behaviors based upon the data and not on a hunch or assumptions. Prescriptive analytics, goes further and suggest actions to benefit from the prediction and also provide decision options to benefit from the predictions and its implications.
Predictive Analytics Value Chain

Predictive Analytics Value Chain
Predictive Analytics Process
1.Define Project:

Define the project outcomes, deliverables, scoping of the effort, business objectives, identify the data sets which are going to be used.
2.Data Collection:

Data Mining for predictive analytics prepares data from multiple sources for analysis. This provides a complete view of the customer interactions.
3. Data Analysis:

Data Analysis is the process of inspecting, cleaning, transforming, and modeling data with the objective of discovering useful information, arriving at conclusions.
4.Statistics:

Statistical Analysis enables to validate the assumptions, hypotheses and test them with using standard statistical models.
5.Modeling:

Predictive Modeling provides the ability to automatically create accurate predictive models about future. There are also options to choose the best solution with multi model evaluation.
6.Deployment:

Predictive Model Deployment provides the option to deploy the analytical results in to the every day decision making process to get results, reports and output by automating the decisions based on the modeling.
7.Model Monitoring:

Models are managed and monitored to review the model performance to ensure that it is providing the results expected.
Predictive Analytics Process

Predictive Analytics Process
Prescriptive Analytics

Prescriptive Analytics automatically automate complex decisions and trade offs to make predictions and then proactively update recommendations based on changing events to take advantage of the prediction.
Applications of Predictive Analytics
1. Customer relationship management (CRM)

Predictive analysis applications are used to achieve CRM objectives such as marketing campaigns, sales, and customer services. Analytical customer relationship management can be applied throughout the customers life cycle, right from acquisition, relationship growth, retention, and win back.
2. Health Care

Predictive analysis applications in health care can determine the patients who are at the risk of developing certain conditions such as diabetes, asthma and other lifetime illnesses. The clinical decision support systems incorporate predictive analytics to support medical decision making at the point of care.
3. Collection Analytics

Predictive analytics applications optimize the allocation of collection resources by identifying the effective collection agencies, contact strategies, legal actions to increase the recovery and also reducing the collection costs.
4. Cross Sell

Predictive analytics applications analyze customers spending, usage and other behavior, leading to efficient cross sales, or selling additional products to current customers for an organization that offers multiple products
5. Fraud detection

Predictive analytics applications can find inaccurate credit applications, fraudulent transactions both done offline and , identity thefts and false insurance claims.
6. Risk management

Predictive analytics applications predicts the best portfolio to maximize return in capital asset pricing model and probabilistic risk assessment to yield accurate forecasts.
7.Direct Marketing

Predictive analytics can also help to identify the most effective combination of product versions, marketing material, communication channels and timing that should be used to target a given consumer.
8.Underwriting

Predictive analytics can help underwrite the quantities by predicting the chances of illness, default, bankruptcy. Predictive analytics can streamline the process of customer acquisition by predicting the future risk behavior of a customer using application level data.
Predicitve Analytics

Predicitve Analytics
Industry Applications

Predictive analytics is used in insurance, banking, marketing, financial services, telecommunications, retail, travel, healthcare, pharmaceuticals, oil and gas and other industries.
1.Predictive Analytics Software

SAS Predictive Analytics, IBM Predictive Analytics, SAP Predictive Analytics, RapidMiner, Angoss Predictive Analytics, GraphLab Create,  SAP InfiniteInsight, FICO Predictive Analytics, Salford Analytics, Oracle Data Mining (ODM), TIMi Suite, TIBCO Analytics, Alteryx Analytics, Alpine Chorus, KNIME, Actian Analytics Platform, Portrait, Predixion, Data Science Studio, H2O, Analytics Solver, STATISTICA, Viscovery Data Mining Suite, Lavastorm Analytics Engine, Rapid Insight Analytics, Advanced Miner, CMSR Data Miner Suite, GMDH Shell, Mathematica, MATLAB, and Minitab are some of the vendors of proprietary predictive analytics solutions in no particular order.

Click on the button below for a review of the top predictive analytics proprietary software solutions.

SAP Predictive Analytics

SAP Predictive Analytics

R, Orange, RapidMiner, Weka, GraphLab Create, Octave, Data Science Studio (DSS), H2O, Lavastorm Public Edition, Tanagra, PredictionIO, HP Distributed R, KNIME, scikit-learn, Actian Analytics Platform, Apache Spark MLlib, Apache Mahout, LIBLINEAR, Vowpal Wabbit, NumPy , and SciPy  are some of the key players in the freeware predictive analytics market in no particular order. Click on the button below for a review of the top predictive analytics freeware software solutions.

R

R
2.Predictive Analytics Software API

Google Prediction API, BigML, Microsoft Azure Machine Learning, Blue Yonder, Swift API, Datagami, GraphLab, Data Science Studio, Apigee Insights, Openscoring.io, Intuitics, Anomaly Detective, Zementis, Predixion, Datumbox Machine Learning Framework, PredictionIO, Logical Glue, Ersatz, H2O, Yottamine, Lattice, InsideView, AgilOne, Futurelytics, Fliptop, RelateIQ, Lumiata, Versium LifeData, Indico, INRIX are some of the Tops Predictive Analytics API in no particular order. Click on the button below for a  review of the top predictive analytics software API solutions:

Google Prediction API

Google Prediction API
3.Predictive Analytics Programs

 data science and predictive analytics programs.

You may also like to review the  business analytics programs list:
4.Predictive Lead Scoring Platforms

Lattice Engines, Fliptop, 6sense, Infer, Leadspace, Mintigo, Salesfusion, Versium, Wise, InsideSales, Lead Liaison, SalesPredict, Custora , Televerde, Futurelytics, Fiserv, Angoss KnowledgeSCORE, KXEN, Predixion, EverString are some of the companies who offer predictive lead scoring platforms in no particular order.

Infer

Infer
5.Predictive Pricing Solutions

Blue Yonder Dynamic Pricing , PROS Pricing Analytics, Zilliant MarginMax, Model N, ECOPA Prezzu, Upstream Commerce Dynamic Pricer, Retalon Dynamic Price Management System are some of the Predictive Pricing Solutions in no particular order.

6.Customer Churn, Renew, Upsell, Cross Sell Software Tools

RapidMiner, Marketo, Preact, Google Prediction API, Lattice Engines, Angoss, PROS Sales Optimizer, Fliptop , Alteryx Analytics, Alpine Chorus, 6sense, Gainsight, GraphLab Create, Predixion Insight, KNIME, Bluenose, Actian, RelateIQ, Zilliant, SalesPredict, Infer, AgilOne, Adobe Recommendations are some of the top customer churn, renew, upsell, cross sell software tools.

More Information on Predictive Analysis Process
Predictive Analytics Process Flow

Predictive Analytics Process Flow

For more information of predictive analytics process, please review the overview of  each components in the predictive analytics process: data collection (data mining), data analysis, statistical analysis, predictive modeling and predictive model deployment.


Predictive analytics is the branch of data mining concerned with the prediction of future probabilities and trends. The central element of predictive analytics is the predictor, a variable that can be measured for an individual or other entity to predict future behavior. For example, an insurance company is likely to take into account potential driving safety predictors such as age, gender, and driving record when issuing car insurance policies.


Multiple predictors are combined into a predictive model, which, when subjected to analysis, can be used to forecast future probabilities with an acceptable level of reliability. In predictive modeling, data is collected, a statistical model is formulated, predictions are made and the model is validated (or revised) as additional data becomes available. Predictive analytics are applied to many research areas, including meteorology, security, genetics, economics, and marketing.

Predictive analytics is a subset of data science. Recognition of the uniqueness of predictive analytics illuminates some interesting needs in research as is illustrated by Table 2.
Table 2. Examples of research in predictive analyticsComparative discipline	Dimension of interest	Predictive analytics research (examples)
Relevant	Less relevant
Statistics	Quantitative	Integrating quantitative and qualitative analysis	Improving Lagrange Multiplier tests for autocorrelation
Forecasting	Predicting the future	Using forecasting techniques for evaluating what would have happened under different circumstances	Deriving generalized estimators of seasonal factors
Optimization	Minimization and maximization	Assessment of the quality of the optimal solution and the ability to implement it versus near optimal solutions	Use of polyhedral functions in linear programming
Discrete event simulation	Quantitative analysis of a system in a stochastic setting	Discrete event simulation in a business process reengineering setting	Random number generation for discrete event simulation
Applied probability	Description of stochastic variables, expected values, and uncertainty	Applied probability along with application anchoring and framing affects from psychology	Asymptotic properties of Gaussian processes
Data mining	Search for patterns and relationships between a large number of variables with lots of data	Data mining preceded by logical and theoretical descriptions of possible relationships and patterns	Gibbs posterior for variable selection in data mining
Analytical mathematical modeling	Precise analysis using artificial and unrealistic assumptions for theorems and proofs	Methods of quickly and inexpensively modeling approximate relationships between variables while still using deductive mathematical methods	Proving inventory theorems that assume known, continuous demand with perfect information

Table 2 examines a sample of disciplines related to predictive analytics, selects a dimension of that discipline, and compares possible research topics and provides an example of a research area that would be more relevant to predictive analytics and an example of a research area that would be less relevant. Table 2 indirectly points to the distinction between predictive analytics and each of these quantitative disciplines. It also provides researchers with possible avenues of research that would be in the realm of predictive analytics.

Importantly, although predictive analytics is related to many long-standing quantitative approaches, it stands as distinct from each. Statistics is quantitative, whereas predictive analytics is both quantitative and qualitative. Forecasting is about predicting the future, and predictive analytics adds questions regarding what would have happened in the past, given different conditions. Optimization is about finding the minimum or maximum of a function, subject to constraints, whereas predictive analytics also concerns what would characterize a system that was not operating optimally. Analytical modeling is primarily about generating mathematical axioms and then proving lemmas and theorems, whereas predictive analytics attempts to quickly and inexpensively approximate relationships between variables while still using deductive mathematical methods to draw conclusions. These are some examples of the differences in emphasis between predictive analytics and well known quantitative disciplines.

The topics in Table 2 have been examined in part, but additional research in these relevant areas would advance predictive analytics' ability to refine and improve supply chain decision making. Indeed, the  of Business Logistics is interested in predictive analytics research that is relevant to logistics and SCM. To that end, we propose definitions of logistics and supply chain predictive analytics:

    Logistics predictive analytics use both quantitative and qualitative methods to estimate the past and future behavior of the flow and storage of inventory, as well as the associated costs and service levels.

    SCM predictive analytics use both quantitative and qualitative methods to improve supply chain design and competitiveness by estimating past and future levels of integration of business processes among functions or companies, as well as the associated costs and service levels.

What is defined here as logistics predictive analytics and SCM predictive analytics has already existed in the past, it just lacked a name. The idea is becoming so common that a name helps with communication about the concept. Reading Table 2 with these definitions in mind should provide a guide to appropriate research on logistics or SCM predictive analytics that would be of particular interest at the  of Business Logistics. Barton and Court (2012) highlight the growing value of advanced analytics:

    “Advanced analytics is likely to become a decisive competitive asset in many industries and a core element in companies' efforts to improve performance. It's a mistake to assume that acquiring the right kind of big data is all that matters. Also essential is developing analytics tools that focus on business outcomes … . (p. 81)




Predictive analytics encompasses a variety of statistical techniques from predictive modeling, machine learning, and data mining that analyze current and historical facts to make predictions about future or otherwise unknown events.[1][2][3]

In business, predictive models exploit patterns found in historical and transactional data to identify risks and opportunities. Models capture relationships among many factors to allow assessment of risk or potential associated with a particular set of conditions, guiding decision making for candidate transactions.[4]

The defining functional effect of these technical approaches is that predictive analytics provides a predictive score (probability) for each individual (customer, employee, healthcare patient, product SKU, vehicle, component, machine, or other organizational unit) in order to determine, inform, or influence organizational processes that pertain across large numbers of individuals, such as in marketing, credit risk assessment, fraud detection, manufacturing, healthcare, and government operations including law enforcement.

Predictive analytics is used in actuarial science,[5] marketing,[1][6] financial services,[7] insurance, telecommunications,[8] retail,[9] travel,[10] healthcare,[11] child protection,[12][13] pharmaceuticals,[14] capacity planning[citation needed] and other fields.

One of the most well known applications is credit scoring,[2] which is used throughout financial services. Scoring models process a customer's credit history, loan application, customer data, etc., in order to rank-order individuals by their likelihood of making future credit payments on time.

Contents

    1 Definition
    2 Types
        2.1 Predictive models
        2.2 Descriptive models
        2.3 Decision models
    3 Applications
        3.1 Analytical customer relationship management (CRM)
        3.2 Child protection
        3.3 Clinical decision support systems
        3.4 Collection analytics
        3.5 Cross-sell
        3.6 Customer retention
        3.7 Direct marketing
        3.8 Fraud detection
        3.9 Portfolio, product or economy-level prediction
        3.10 Risk management
        3.11 Underwriting
    4 Technology and big data influences
    5 Analytical Techniques
        5.1 Regression techniques
            5.1.1 Linear regression model
            5.1.2 Discrete choice models
            5.1.3 Logistic regression
            5.1.4 Multinomial logistic regression
            5.1.5 Probit regression
            5.1.6 Logit versus probit
            5.1.7 Time series models
            5.1.8 Survival or duration analysis
            5.1.9 Classification and regression trees (CART)
            5.1.10 Multivariate adaptive regression splines
        5.2 Machine learning techniques
            5.2.1 Neural networks
            5.2.2 Multilayer Perceptron (MLP)
            5.2.3 Radial basis functions
            5.2.4 Support vector machines
            5.2.5 Naïve Bayes
            5.2.6 k-nearest neighbours
            5.2.7 Geospatial predictive modeling
    6 Tools
        6.1 PMML
    7 Criticism
    8 See also
    9 References
    10 Further reading

Definition

Predictive analytics is an area of data mining that deals with extracting information from data and using it to predict trends and behavior patterns.[1] Often the unknown event of interest is in the future, but predictive analytics can be applied to any type of unknown whether it be in the past, present or future. For example, identifying suspects after a crime has been committed, or credit card fraud as it occurs.[15] The core of predictive analytics relies on capturing relationships between explanatory variables and the predicted variables from past occurrences, and exploiting them to predict the unknown outcome. It is important to note, however, that the accuracy and usability of results will depend greatly on the level of data analysis and the quality of assumptions.

Predictive analytics is often defined as predicting at a more detailed level of granularity, i.e., generating predictive scores (probabilities) for each individual organizational element. This distinguishes it from forecasting. For example, "Predictive analytics—Technology that learns from experience (data) to predict the future behavior of individuals in order to drive better decisions."[16] In the future industrial systems the value of Predictive Analytics is to predict and prevent potential issues to achieve near-zero break-down and further be integrated into prescriptive analytics for decision optimization. Furthermore, the converted data can be used for closed-loop product life cycle improvement[17] which is the vision of Industrial Internet Consortium.
Types

Generally, the term predictive analytics is used to mean predictive modeling, "scoring" data with predictive models, and forecasting. However, people are increasingly using the term to refer to related analytical disciplines, such as descriptive modeling and decision modeling or optimization. These disciplines also involve rigorous data analysis, and are widely used in business for segmentation and decision making, but have different purposes and the statistical techniques underlying them vary.
Predictive models

Predictive models are models of the relation between the specific performance of a unit in a sample and one or more known attributes or features of the unit. The objective of the model is to assess the likelihood that a similar unit in a different sample will exhibit the specific performance. This category encompasses models in many areas, such as marketing, where they seek out subtle data patterns to answer questions about customer performance, or fraud detection models. Predictive models often perform calculations during live transactions, for example, to evaluate the risk or opportunity of a given customer or transaction, in order to guide a decision. With advancements in computing speed, individual agent modeling systems have become capable of simulating human behaviour or reactions to given stimuli or scenarios.

The available sample units with known attributes and known performances is referred to as the “training sample.” The units in other samples, with known attributes but unknown performances, are referred to as “out of [training] sample” units. The out of sample bear no chronological relation to the training sample units. For example, the training sample may consists of literary attributes of writings by Victorian authors, with known attribution, and the out-of sample unit may be newly found writing with unknown authorship; a predictive model may aid in attributing a work to a known author. Another example is given by analysis of blood splatter in simulated crime scenes in which the out of sample unit is the actual blood splatter pattern from a crime scene. The out of sample unit may be from the same time as the training units, from a previous time, or from a future time.
Descriptive models

Descriptive models quantify relationships in data in a way that is often used to classify customers or prospects into groups. Unlike predictive models that focus on predicting a single customer behavior (such as credit risk), descriptive models identify many different relationships between customers or products. Descriptive models do not rank-order customers by their likelihood of taking a particular action the way predictive models do. Instead, descriptive models can be used, for example, to categorize customers by their product preferences and life stage. Descriptive modeling tools can be utilized to develop further models that can simulate large number of individualized agents and make predictions.
Decision models

Decision models describe the relationship between all the elements of a decision — the known data (including results of predictive models), the decision, and the forecast results of the decision — in order to predict the results of decisions involving many variables. These models can be used in optimization, maximizing certain outcomes while minimizing others. Decision models are generally used to develop decision logic or a set of business rules that will produce the desired action for every customer or circumstance.
Applications

Although predictive analytics can be put to use in many applications, we outline a few examples where predictive analytics has shown positive impact in recent years.
Analytical customer relationship management (CRM)

Analytical Customer Relationship Management is a frequent commercial application of Predictive Analysis. Methods of predictive analysis are applied to customer data to pursue CRM objectives, which involve constructing a holistic view of the customer no matter where their information resides in the company or the department involved. CRM uses predictive analysis in applications for marketing campaigns, sales, and customer services to name a few. These tools are required in order for a company to posture and focus their efforts effectively across the breadth of their customer base. They must analyze and understand the products in demand or have the potential for high demand, predict customers' buying habits in order to promote relevant products at multiple touch points, and proactively identify and mitigate issues that have the potential to lose customers or reduce their ability to gain new ones. Analytical Customer Relationship Management can be applied throughout the customers lifecycle (acquisition, relationship growth, retention, and win-back). Several of the application areas described below (direct marketing, cross-sell, customer retention) are part of Customer Relationship Managements.
Child protection

Over the last 5 years, some Child Welfare Agencies have started using predictive analytics to flag high risk cases.[18] The approach has been called "innovative" by the Commission to Eliminate Child Abuse and Neglect Fatalities (CECANF),[19] and in Hillsborough County, FL, where the Lead Child Welfare Agency uses a predictive modeling tool called Eckerd Rapid Safety Feedback®, there have been no abuse-related child deaths in the target population as of this writing.[20]
Clinical decision support systems

Experts use predictive analysis in health care primarily to determine which patients are at risk of developing certain conditions, like diabetes, asthma, heart disease, and other lifetime illnesses. Additionally, sophisticated clinical decision support systems incorporate predictive analytics to support medical decision making at the point of care. A working definition has been proposed by Robert Hayward of the Centre for Health Evidence: "Clinical Decision Support Systems link health observations with health knowledge to influence health choices by clinicians for improved health care."[citation needed]
Collection analytics

Many portfolios have a set of delinquent customers who do not make their payments on time. The financial institution has to undertake collection activities on these customers to recover the amounts due. A lot of collection resources are wasted on customers who are difficult or impossible to recover. Predictive analytics can help optimize the allocation of collection resources by identifying the most effective collection agencies, contact strategies, legal actions and other strategies to each customer, thus significantly increasing recovery at the same time reducing collection costs.
Cross-sell

Often corporate organizations collect and maintain abundant data (e.g. customer records, sale transactions) as exploiting hidden relationships in the data can provide a competitive advantage. For an organization that offers multiple products, predictive analytics can help analyze customers' spending, usage and other behavior, leading to efficient cross sales, or selling additional products to current customers.[3] This directly leads to higher profitability per customer and stronger customer relationships.
Customer retention

With the number of competing services available, businesses need to focus efforts on maintaining continuous consumer satisfaction, rewarding consumer loyalty and minimizing customer attrition. In addition, small increases in customer retention have been shown to increase profits disproportionately. One study concluded that a 5% increase in customer retention rates will increase profits by 25% to 95%.[21] Businesses tend to respond to customer attrition on a reactive basis, acting only after the customer has initiated the process to terminate service. At this stage, the chance of changing the customer's decision is almost zero. Proper application of predictive analytics can lead to a more proactive retention strategy. By a frequent examination of a customer’s past service usage, service performance, spending and other behavior patterns, predictive models can determine the likelihood of a customer terminating service sometime soon.[8] An intervention with lucrative offers can increase the chance of retaining the customer. Silent attrition, the behavior of a customer to slowly but steadily reduce usage, is another problem that many companies face. Predictive analytics can also predict this behavior, so that the company can take proper actions to increase customer activity.
Direct marketing

When marketing consumer products and services, there is the challenge of keeping up with competing products and consumer behavior. Apart from identifying prospects, predictive analytics can also help to identify the most effective combination of product versions, marketing material, communication channels and timing that should be used to target a given consumer. The  of predictive analytics is typically to lower the cost per order or cost per action.
Fraud detection

Fraud is a big problem for many businesses and can be of various types: inaccurate credit applications, fraudulent transactions (both offline and ), identity thefts and false insurance claims. These problems plague firms of all sizes in many industries. Some examples of likely victims are credit card issuers, insurance companies,[22] retail merchants, manufacturers, business-to-business suppliers and even services providers. A predictive model can help weed out the "bads" and reduce a business's exposure to fraud.

Predictive modeling can also be used to identify high-risk fraud candidates in business or the public sector. Mark Nigrini developed a risk-scoring method to identify audit targets. He describes the use of this approach to detect fraud in the franchisee sales reports of an international fast-food chain. Each location is scored using 10 predictors. The 10 scores are then weighted to give one final overall risk score for each location. The same scoring approach was also used to identify high-risk check kiting accounts, potentially fraudulent travel agents, and questionable vendors. A reasonably complex model was used to identify fraudulent monthly reports submitted by divisional controllers.[23]

The Internal Revenue Service (IRS) of the United States also uses predictive analytics to mine tax returns and identify tax fraud.[22]

Recent[when?] advancements in technology have also introduced predictive behavior analysis for web fraud detection. This type of solution utilizes heuristics in order to study normal web user behavior and detect anomalies indicating fraud attempts.
Portfolio, product or economy-level prediction

Often the focus of analysis is not the consumer but the product, portfolio, firm, industry or even the economy. For example, a retailer might be interested in predicting store-level demand for inventory management purposes. Or the Federal Reserve Board might be interested in predicting the unemployment rate for the next year. These types of problems can be addressed by predictive analytics using time series techniques (see below). They can also be addressed via machine learning approaches which transform the original time series into a feature vector space, where the learning algorithm finds patterns that have predictive power.[24][25]
Risk management

When employing risk management techniques, the results are always to predict and benefit from a future scenario. The Capital asset pricing model (CAP-M) "predicts" the best portfolio to maximize return, Probabilistic Risk Assessment (PRA)--when combined with mini-Delphi Techniques and statistical approaches yields accurate forecasts and RiskAoA is a stand-alone predictive tool.[26] These are three examples of approaches that can extend from project to market, and from near to long term. Underwriting (see below) and other business approaches identify risk management as a predictive method.
Underwriting

Many businesses have to account for risk exposure due to their different services and determine the cost needed to cover the risk. For example, auto insurance providers need to accurately determine the amount of premium to charge to cover each automobile and driver. A financial company needs to assess a borrower's potential and ability to pay before granting a loan. For a health insurance provider, predictive analytics can analyze a few years of past medical claims data, as well as lab, pharmacy and other records where available, to predict how expensive an enrollee is likely to be in the future. Predictive analytics can help underwrite these quantities by predicting the chances of illness, default, bankruptcy, etc. Predictive analytics can streamline the process of customer acquisition by predicting the future risk behavior of a customer using application level data.[5] Predictive analytics in the form of credit scores have reduced the amount of time it takes for loan approvals, especially in the mortgage market where lending decisions are now made in a matter of hours rather than days or even weeks. Proper predictive analytics can lead to proper pricing decisions, which can help mitigate future risk of default.
Technology and big data influences

Big data is a collection of data sets that are so large and complex that they become awkward to work with using traditional database management tools. The volume, variety and velocity of big data have introduced challenges across the board for capture, storage, search, sharing, analysis, and visualization. Examples of big data sources include web logs, RFID, sensor data, social networks, Internet search indexing, call detail records, military surveillance, and complex data in astronomic, biogeochemical, genomics, and atmospheric sciences. Big Data is the core of most predictive analytic services offered by IT organizations.[27] Thanks to technological advances in computer hardware — faster CPUs, cheaper memory, and MPP architectures — and new technologies such as Hadoop, MapReduce, and in-database and text analytics for processing big data, it is now feasible to collect, analyze, and mine massive amounts of structured and unstructured data for new insights.[22] It is also possible to run predictive algorithms on streaming data.[28] Today, exploring big data and using predictive analytics is within reach of more organizations than ever before and new methods that are capable for handling such datasets are proposed [29] [30]
Analytical Techniques

The approaches and techniques used to conduct predictive analytics can broadly be grouped into regression techniques and machine learning techniques.
Regression techniques

Regression models are the mainstay of predictive analytics. The focus lies on establishing a mathematical equation as a model to represent the interactions between the different variables in consideration. Depending on the situation, there are a wide variety of models that can be applied while performing predictive analytics. Some of them are briefly discussed below.
Linear regression model

The linear regression model analyzes the relationship between the response or dependent variable and a set of independent or predictor variables. This relationship is expressed as an equation that predicts the response variable as a linear function of the parameters. These parameters are adjusted so that a measure of fit is optimized. Much of the effort in model fitting is focused on minimizing the size of the residual, as well as ensuring that it is randomly distributed with respect to the model predictions.

The  of regression is to select the parameters of the model so as to minimize the sum of the squared residuals. This is referred to as ordinary least squares (OLS) estimation and results in best linear unbiased estimates (BLUE) of the parameters if and only if the Gauss-Markov assumptions are satisfied.

Once the model has been estimated we would be interested to know if the predictor variables belong in the model – i.e. is the estimate of each variable's contribution reliable? To do this we can check the statistical significance of the model’s coefficients which can be measured using the t-statistic. This amounts to testing whether the coefficient is significantly different from zero. How well the model predicts the dependent variable based on the value of the independent variables can be assessed by using the R² statistic. It measures predictive power of the model i.e. the proportion of the total variation in the dependent variable that is "explained" (accounted for) by variation in the independent variables.
Discrete choice models

Multivariate regression (above) is generally used when the response variable is continuous and has an unbounded range. Often the response variable may not be continuous but rather discrete. While mathematically it is feasible to apply multivariate regression to discrete ordered dependent variables, some of the assumptions behind the theory of multivariate linear regression no longer hold, and there are other techniques such as discrete choice models which are better suited for this type of analysis. If the dependent variable is discrete, some of those superior methods are logistic regression, multinomial logit and probit models. Logistic regression and probit models are used when the dependent variable is binary.
Logistic regression
For more details on this topic, see logistic regression.

In a classification setting, assigning outcome probabilities to observations can be achieved through the use of a logistic model, which is basically a method which transforms information about the binary dependent variable into an unbounded continuous variable and estimates a regular multivariate model (See Allison's Logistic Regression for more information on the theory of Logistic Regression).

The Wald and likelihood-ratio test are used to test the statistical significance of each coefficient b in the model (analogous to the t tests used in OLS regression; see above). A test assessing the goodness-of-fit of a classification model is the "percentage correctly predicted".
Multinomial logistic regression

An extension of the binary logit model to cases where the dependent variable has more than 2 categories is the multinomial logit model. In such cases collapsing the data into two categories might not make good sense or may lead to loss in the richness of the data. The multinomial logit model is the appropriate technique in these cases, especially when the dependent variable categories are not ordered (for examples colors like red, blue, green). Some authors have extended multinomial regression to include feature selection/importance methods such as Random multinomial logit.
Probit regression

Probit models offer an alternative to logistic regression for modeling categorical dependent variables. Even though the outcomes tend to be similar, the underlying distributions are different. Probit models are popular in social sciences like economics.

A good way to understand the key difference between probit and logit models is to assume that the dependent variable is driven by a latent variable z, which is a sum of a linear combination of explanatory variables and a random noise term.

We do not observe z but instead observe y which takes the value 0 (when z < 0) or 1 (otherwise). In the logit model we assume that the random noise term follows a logistic distribution with mean zero. In the probit model we assume that it follows a normal distribution with mean zero. Note that in social sciences (e.g. economics), probit is often used to model situations where the observed variable y is continuous but takes values between 0 and 1.
Logit versus probit

The Probit model has been around longer than the logit model. They behave similarly, except that the logistic distribution tends to be slightly flatter tailed. One of the reasons the logit model was formulated was that the probit model was computationally difficult due to the requirement of numerically calculating integrals. Modern computing however has made this computation fairly simple. The coefficients obtained from the logit and probit model are fairly close. However, the odds ratio is easier to interpret in the logit model.

Practical reasons for choosing the probit model over the logistic model would be:

    There is a strong belief that the underlying distribution is normal
    The actual event is not a binary outcome (e.g., bankruptcy status) but a proportion (e.g., proportion of population at different debt levels).

Time series models

Time series models are used for predicting or forecasting the future behavior of variables. These models account for the fact that data points taken over time may have an internal structure (such as autocorrelation, trend or seasonal variation) that should be accounted for. As a result, standard regression techniques cannot be applied to time series data and methodology has been developed to decompose the trend, seasonal and cyclical component of the series. Modeling the dynamic path of a variable can improve forecasts since the predictable component of the series can be projected into the future.

Time series models estimate difference equations containing stochastic components. Two commonly used forms of these models are autoregressive models (AR) and moving-average (MA) models. The Box–Jenkins methodology (1976) developed by George Box and G.M. Jenkins combines the AR and MA models to produce the ARMA (autoregressive moving average) model which is the cornerstone of stationary time series analysis. ARIMA (autoregressive integrated moving average models) on the other hand are used to describe non-stationary time series. Box and Jenkins suggest differencing a non stationary time series to obtain a stationary series to which an ARMA model can be applied. Non stationary time series have a pronounced trend and do not have a constant long-run mean or variance.

Box and Jenkins proposed a three-stage methodology which includes: model identification, estimation and validation. The identification stage involves identifying if the series is stationary or not and the presence of seasonality by examining plots of the series, autocorrelation and partial autocorrelation functions. In the estimation stage, models are estimated using non-linear time series or maximum likelihood estimation procedures. Finally the validation stage involves diagnostic checking such as plotting the residuals to detect outliers and evidence of model fit.

In recent years time series models have become more sophisticated and attempt to model conditional heteroskedasticity with models such as ARCH (autoregressive conditional heteroskedasticity) and GARCH (generalized autoregressive conditional heteroskedasticity) models frequently used for financial time series. In addition time series models are also used to understand inter-relationships among economic variables represented by systems of equations using VAR (vector autoregression) and structural VAR models.
Survival or duration analysis

Survival analysis is another name for time to event analysis. These techniques were primarily developed in the medical and biological sciences, but they are also widely used in the social sciences like economics, as well as in engineering (reliability and failure time analysis).

Censoring and non-normality, which are characteristic of survival data, generate difficulty when trying to analyze the data using conventional statistical models such as multiple linear regression. The normal distribution, being a symmetric distribution, takes positive as well as negative values, but duration by its very nature cannot be negative and therefore normality cannot be assumed when dealing with duration/survival data. Hence the normality assumption of regression models is violated.

The assumption is that if the data were not censored it would be representative of the population of interest. In survival analysis, censored observations arise whenever the dependent variable of interest represents the time to a terminal event, and the duration of the study is limited in time.

An important concept in survival analysis is the hazard rate, defined as the probability that the event will occur at time t conditional on surviving until time t. Another concept related to the hazard rate is the survival function which can be defined as the probability of surviving to time t.

Most models try to model the hazard rate by choosing the underlying distribution depending on the shape of the hazard function. A distribution whose hazard function slopes upward is said to have positive duration dependence, a decreasing hazard shows negative duration dependence whereas constant hazard is a process with no memory usually characterized by the exponential distribution. Some of the distributional choices in survival models are: F, gamma, Weibull, log normal, inverse normal, exponential etc. All these distributions are for a non-negative random variable.

Duration models can be parametric, non-parametric or semi-parametric. Some of the models commonly used are Kaplan-Meier and Cox proportional hazard model (non parametric).
Classification and regression trees (CART)
Main article: decision tree learning

Globally-optimal classification tree analysis (GO-CTA) (also called hierarchical optimal discriminant analysis) is a generalization of optimal discriminant analysis that may be used to identify the statistical model that has maximum accuracy for predicting the value of a categorical dependent variable for a dataset consisting of categorical and continuous variables. The output of HODA is a non-orthogonal tree that combines categorical variables and cut points for continuous variables that yields maximum predictive accuracy, an assessment of the exact Type I error rate, and an evaluation of potential cross-generalizability of the statistical model. Hierarchical optimal discriminant analysis may be thought of as a generalization of Fisher's linear discriminant analysis. Optimal discriminant analysis is an alternative to ANOVA (analysis of variance) and regression analysis, which attempt to express one dependent variable as a linear combination of other features or measurements. However, ANOVA and regression analysis give a dependent variable that is a numerical variable, while hierarchical optimal discriminant analysis gives a dependent variable that is a class variable.

Classification and regression trees (CART) are a non-parametric decision tree learning technique that produces either classification or regression trees, depending on whether the dependent variable is categorical or numeric, respectively.

Decision trees are formed by a collection of rules based on variables in the modeling data set:

    Rules based on variables' values are selected to get the best split to differentiate observations based on the dependent variable
    Once a rule is selected and splits a node into two, the same process is applied to each "child" node (i.e. it is a recursive procedure)
    Splitting stops when CART detects no further gain can be made, or some pre-set stopping rules are met. (Alternatively, the data are split as much as possible and then the tree is later pruned.)

Each branch of the tree ends in a terminal node. Each observation falls into one and exactly one terminal node, and each terminal node is uniquely defined by a set of rules.

A very popular method for predictive analytics is Leo Breiman's Random forests.
Multivariate adaptive regression splines

Multivariate adaptive regression splines (MARS) is a non-parametric technique that builds flexible models by fitting piecewise linear regressions.

An important concept associated with regression splines is that of a knot. Knot is where one local regression model gives way to another and thus is the point of intersection between two splines.

In multivariate and adaptive regression splines, basis functions are the tool used for generalizing the search for knots. Basis functions are a set of functions used to represent the information contained in one or more variables. Multivariate and Adaptive Regression Splines model almost always creates the basis functions in pairs.

Multivariate and adaptive regression spline approach deliberately overfits the model and then prunes to get to the optimal model. The algorithm is computationally very intensive and in practice we are required to specify an upper limit on the number of basis functions.
Machine learning techniques

Machine learning, a branch of artificial intelligence, was originally employed to develop techniques to enable computers to learn. Today, since it includes a number of advanced statistical methods for regression and classification, it finds application in a wide variety of fields including medical diagnostics, credit card fraud detection, face and speech recognition and analysis of the stock market. In certain applications it is sufficient to directly predict the dependent variable without focusing on the underlying relationships between variables. In other cases, the underlying relationships can be very complex and the mathematical form of the dependencies unknown. For such cases, machine learning techniques emulate human cognition and learn from training examples to predict future events.

A brief discussion of some of these methods used commonly for predictive analytics is provided below. A detailed study of machine learning can be found in Mitchell (1997).
Neural networks

Neural networks are nar sophisticated modeling techniques that are able to model complex functions. They can be applied to problems of prediction, classification or control in a wide spectrum of fields such as finance, cognitive psychology/neuroscience, medicine, engineering, and physics.

Neural networks are used when the exact nature of the relationship between inputs and output is not known. A key feature of neural networks is that they learn the relationship between inputs and output through training. There are three types of training in neural networks used by different networks, supervised and unsupervised training, reinforcement learning, with supervised being the most common one.

Some examples of neural network training techniques are backpropagation, quick propagation, conjugate gradient descent, projection operator, Delta-Bar-Delta etc. Some unsupervised network architectures are multilayer perceptrons, Kohonen networks, Hopfield networks, etc.
Multilayer Perceptron (MLP)

The Multilayer Perceptron (MLP) consists of an input and an output layer with one or more hidden layers of narly-activating nodes or sigmoid nodes. This is determined by the weight vector and it is necessary to adjust the weights of the network. The backpropagation employs gradient fall to minimize the squared error between the network output values and desired values for those outputs. The weights adjusted by an iterative process of repetitive present of attributes. Small changes in the weight to get the desired values are done by the process called training the net and is done by the training set (learning rule).
Radial basis functions

A radial basis function (RBF) is a function which has built into it a distance criterion with respect to a center. Such functions can be used very efficiently for interpolation and for smoothing of data. Radial basis functions have been applied in the area of neural networks where they are used as a replacement for the sigmoidal transfer function. Such networks have 3 layers, the input layer, the hidden layer with the RBF non-linearity and a linear output layer. The most popular choice for the non-linearity is the Gaussian. RBF networks have the advantage of not being locked into local minima as do the feed-forward networks such as the multilayer perceptron.
Support vector machines

Support Vector Machines (SVM) are used to detect and exploit complex patterns in data by clustering, classifying and ranking the data. They are learning machines that are used to perform binary classifications and regression estimations. They commonly use kernel based methods to apply linear classification techniques to non-linear classification problems. There are a number of types of SVM such as linear, polynomial, sigmoid etc.
Naïve Bayes

Naïve Bayes based on Bayes conditional probability rule is used for performing classification tasks. Naïve Bayes assumes the predictors are statistically independent which makes it an effective classification tool that is easy to interpret. It is best employed when faced with the problem of ‘curse of dimensionality’ i.e. when the number of predictors is very high.
k-nearest neighbours

The nearest neighbour algorithm (KNN) belongs to the class of pattern recognition statistical methods. The method does not impose a priori any assumptions about the distribution from which the modeling sample is drawn. It involves a training set with both positive and negative values. A new sample is classified by calculating the distance to the nearest neighbouring training case. The sign of that point will determine the classification of the sample. In the k-nearest neighbour classifier, the k nearest points are considered and the sign of the majority is used to classify the sample. The performance of the kNN algorithm is influenced by three main factors: (1) the distance measure used to locate the nearest neighbours; (2) the decision rule used to derive a classification from the k-nearest neighbours; and (3) the number of neighbours used to classify the new sample. It can be proved that, unlike other methods, this method is universally asymptotically convergent, i.e.: as the size of the training set increases, if the observations are independent and identically distributed (i.i.d.), regardless of the distribution from which the sample is drawn, the predicted class will converge to the class assignment that minimizes misclassification error. See Devroy et al.
Geospatial predictive modeling

Conceptually, geospatial predictive modeling is rooted in the principle that the occurrences of events being modeled are limited in distribution. Occurrences of events are neither uniform nor random in distribution – there are spatial environment factors (infrastructure, sociocultural, topographic, etc.) that constrain and influence where the locations of events occur. Geospatial predictive modeling attempts to describe those constraints and influences by spatially correlating occurrences of historical geospatial locations with environmental factors that represent those constraints and influences. Geospatial predictive modeling is a process for analyzing events through a geographic filter in order to make statements of likelihood for event occurrence or emergence.
Tools

Historically, using predictive analytics tools—as well as understanding the results they delivered—required advanced skills. However, modern predictive analytics tools are no longer restricted to IT specialists[citation needed]. As more organizations adopt predictive analytics into decision-making processes and integrate it into their operations, they are creating a shift in the market toward business users as the primary consumers of the information. Business users want tools they can use on their own. Vendors are responding by creating new software that removes the mathematical complexity, provides user-friendly graphic interfaces and/or builds in short cuts that can, for example, recognize the kind of data available and suggest an appropriate predictive model.[31] Predictive analytics tools have become sophisticated enough to adequately present and dissect data problems[citation needed], so that any data-savvy information worker can utilize them to analyze data and retrieve meaningful, useful results.[3] For example, modern tools present findings using simple charts, graphs, and scores that indicate the likelihood of possible outcomes.[32]

There are numerous tools available in the marketplace that help with the execution of predictive analytics. These range from those that need very little user sophistication to those that are designed for the expert practitioner. The difference between these tools is often in the level of customization and heavy data lifting allowed.

Notable open source predictive analytic tools include:

    Apache Mahout
    GNU Octave
    KNIME
    OpenNN
    Orange
    R
    RiskAoA
    scikit-learn
    Weka

Notable commercial predictive analytic tools include:

    Alpine Data Labs
    Angoss KnowledgeSTUDIO
    BIRT Analytics
    IBM SPSS Statistics and IBM SPSS Modeler
    KXEN Modeler
    Mathematica
    MATLAB
    Minitab
    LabVIEW[33]
    Neural Designer
    Oracle Advanced Analytics
    Pervasive
    Predixion Software
    RapidMiner
    RCASE
    Revolution Analytics
    SAP HANA[34] and SAP BusinessObjects Predictive Analytics[35]
    SAS and SAS Enterprise Miner
    STATA
    Statgraphics
    STATISTICA
    TeleRetail
    TIBCO

Beside these software packages, specific tools have also been developed for industrial applications. For example, Watchdog Agent Toolbox has been developed and optimized for predictive analysis in prognostics and health management applications and is available for MATLAB and LABVIEW[36][37]

The most popular commercial predictive analytics software packages according to the Rexer Analytics Survey for 2013 are IBM SPSS Modeler, SAS Enterprise Miner, and Dell Statistica <http://www.rexeranalytics.com/Data-Miner-Survey-2013-Intro.html 
>.
PMML

In an attempt to provide a standard language for expressing predictive models, the Predictive Model Markup Language (PMML) has been proposed. Such an XML-based language provides a way for the different tools to define predictive models and to share these between PMML compliant applications. PMML 4.0 was released in June, 2009.
Criticism

There are plenty of skeptics when it comes to computers and algorithms abilities to predict the future, including Gary King, a professor from Harvard  and the director of the Institute for Quantitative Social Science. [38] People are influenced by their environment in innumerable ways. Trying to understand what people will do next assumes that all the influential variables can be known and measured accurately. "People's environments change even more quickly than they themselves do. Everything from the weather to their relationship with their mother can change the way people think and act. All of those variables are unpredictable. How they will impact a person is even less predictable. If put in the exact same situation tomorrow, they may make a completely different decision. This means that a statistical prediction is only valid in sterile laboratory conditions, which suddenly isn't as useful as it seemed before." [39]
See also

    Criminal Reduction Utilising Statistical History
    Data mining
    Learning analytics
    Odds algorithm
    Pattern recognition
    Prescriptive analytics
    Predictive modeling
    RiskAoA a predictive tool for discriminating future decisions.Predictive analytics allows you to discover, analyze and act on data. It’s about learning from the past to uncover trends and predict likely outcomes. But that’s not all. Predictive analytics gives you a framework to analyze data over time, leading to more refined outcomes and corrective actions.

And using data to create a coherent view of the future has never been more important. The convergence of big data, time series data, social media, sensor data and mobile devices gives you the potential to add more fuel to your predictive analytics engine. Now it’s up to you to collect, manage and analyze this information – and position your organization for success.
Analytics Insights
Analytics Insights
Connect with the latest insights on analytics through related articles and research.
More on predictive analytics

    Drive your business with predictive analytics
    Three steps to putting predictive analytics to work

How it works

Predictive analytics combines techniques from statistics, data mining and machine learning to find meaning from large amounts of data. Whether you’re in marketing, compliance, customer service, operations or any other business unit, your data can show where you are – and predict where you’re going.

How do organizations approach predictive analytics? The key stages for an analytical life cycle include:

    Analytical data preparation – Source, clean and prepare the data for optimal results.
    Visualization and exploration – Explore all data to identify relevant variables, trends and relationships.
    Statistical analysis – Use everything from simple descriptive statistics to complex Bayesian analysis to quantify uncertainty, make inferences and drive decisions.
    Predictive modeling – Build the predictive model using statistical, data mining or text mining algorithms, including the critical capability of transforming and selecting key variables.
    Model deployment – Apply the new champion model, once validated and approved, to new data.
    Model management and monitoring – Examine model performance to make sure it is up-to-date and delivering valid results.

Predictive analytics in action

Marketing – From telecommunications to education to gaming and beyond, organizations need to forecast customer responses or purchases. Predictive models enable businesses to discover and attract the most profitable customers, helping maximize value from their marketing budget.

Risk – Credit scores assess a buyer’s likelihood to default on purchases like cars, homes or insurance. Credit scores are numbers generated by a predictive model that incorporates all data relevant to creditworthiness. Predictive analytics has other risk-related uses as well, including claims, collections, fraud and security.

Operations – To make an organization more efficient, you need to understand future needs and anticipate demand. Manufacturers need to manage inventory and factory resources. Airlines must decide how many tickets to sell at each price for a flight. Hotels try to predict the number of guests they can expect on a given night. Predictive analytics is at the heart of all of these operational decisions.


What is Predictive Analytics? Predictive analytics is the branch of the advanced analytics which is used to make predictions about unknown future events. Predictive analytics uses many techniques from data mining, statistics, modeling, machine learning, and artificial intelligence to analyze current data to make predictions about future. It uses a number of data mining, predictive modeling and analytical techniques to bring together the management, information technology, and modeling business process to make predictions about future. The patterns found in historical and transactional data can be used to identify risks and opportunities for future. Predictive analytics models capture relationships among many factors to assess risk with a particular set of conditions to assign a score, or weightage. By successfully applying predictive analytics the businesses can effectively interpret big data for their benefit.

The data mining and text analytics along with statistics, allows the business users to create predictive intelligence by uncovering patterns and relationships in both the structured and unstructured data. The data which can be used readily for analysis are structured data, examples like age, gender, marital status, income, sales. Unstructured data are textual data in call center notes, social media content, or other type of open text which need to be extracted from the text, along with the sentiment, and then used in the model building process.

Predictive analytics allows organizations to become proactive, forward looking, anticipating outcomes and behaviors based upon the data and not on a hunch or assumptions. Prescriptive analytics, goes further and suggest actions to benefit from the prediction and also provide decision options to benefit from the predictions and its implications.
Predictive Analytics Value Chain

Predictive Analytics Value Chain
Predictive Analytics Process
1.Define Project:

Define the project outcomes, deliverables, scoping of the effort, business objectives, identify the data sets which are going to be used.
2.Data Collection:

Data Mining for predictive analytics prepares data from multiple sources for analysis. This provides a complete view of the customer interactions.
3. Data Analysis:

Data Analysis is the process of inspecting, cleaning, transforming, and modeling data with the objective of discovering useful information, arriving at conclusions.
4.Statistics:

Statistical Analysis enables to validate the assumptions, hypotheses and test them with using standard statistical models.
5.Modeling:

Predictive Modeling provides the ability to automatically create accurate predictive models about future. There are also options to choose the best solution with multi model evaluation.
6.Deployment:

Predictive Model Deployment provides the option to deploy the analytical results in to the every day decision making process to get results, reports and output by automating the decisions based on the modeling.
7.Model Monitoring:

Models are managed and monitored to review the model performance to ensure that it is providing the results expected.
Predictive Analytics Process

Predictive Analytics Process
Prescriptive Analytics

Prescriptive Analytics automatically automate complex decisions and trade offs to make predictions and then proactively update recommendations based on changing events to take advantage of the prediction.
Applications of Predictive Analytics
1. Customer relationship management (CRM)

Predictive analysis applications are used to achieve CRM objectives such as marketing campaigns, sales, and customer services. Analytical customer relationship management can be applied throughout the customers life cycle, right from acquisition, relationship growth, retention, and win back.
2. Health Care

Predictive analysis applications in health care can determine the patients who are at the risk of developing certain conditions such as diabetes, asthma and other lifetime illnesses. The clinical decision support systems incorporate predictive analytics to support medical decision making at the point of care.
3. Collection Analytics

Predictive analytics applications optimize the allocation of collection resources by identifying the effective collection agencies, contact strategies, legal actions to increase the recovery and also reducing the collection costs.
4. Cross Sell

Predictive analytics applications analyze customers spending, usage and other behavior, leading to efficient cross sales, or selling additional products to current customers for an organization that offers multiple products
5. Fraud detection

Predictive analytics applications can find inaccurate credit applications, fraudulent transactions both done offline and , identity thefts and false insurance claims.
6. Risk management

Predictive analytics applications predicts the best portfolio to maximize return in capital asset pricing model and probabilistic risk assessment to yield accurate forecasts.
7.Direct Marketing

Predictive analytics can also help to identify the most effective combination of product versions, marketing material, communication channels and timing that should be used to target a given consumer.
8.Underwriting

Predictive analytics can help underwrite the quantities by predicting the chances of illness, default, bankruptcy. Predictive analytics can streamline the process of customer acquisition by predicting the future risk behavior of a customer using application level data.
Predicitve Analytics

Predicitve Analytics
Industry Applications

Predictive analytics is used in insurance, banking, marketing, financial services, telecommunications, retail, travel, healthcare, pharmaceuticals, oil and gas and other industries.
1.Predictive Analytics Software

SAS Predictive Analytics, IBM Predictive Analytics, SAP Predictive Analytics, RapidMiner, Angoss Predictive Analytics, GraphLab Create,  SAP InfiniteInsight, FICO Predictive Analytics, Salford Analytics, Oracle Data Mining (ODM), TIMi Suite, TIBCO Analytics, Alteryx Analytics, Alpine Chorus, KNIME, Actian Analytics Platform, Portrait, Predixion, Data Science Studio, H2O, Analytics Solver, STATISTICA, Viscovery Data Mining Suite, Lavastorm Analytics Engine, Rapid Insight Analytics, Advanced Miner, CMSR Data Miner Suite, GMDH Shell, Mathematica, MATLAB, and Minitab are some of the vendors of proprietary predictive analytics solutions in no particular order.

Click on the button below for a review of the top predictive analytics proprietary software solutions.

SAP Predictive Analytics

SAP Predictive Analytics

R, Orange, RapidMiner, Weka, GraphLab Create, Octave, Data Science Studio (DSS), H2O, Lavastorm Public Edition, Tanagra, PredictionIO, HP Distributed R, KNIME, scikit-learn, Actian Analytics Platform, Apache Spark MLlib, Apache Mahout, LIBLINEAR, Vowpal Wabbit, NumPy , and SciPy  are some of the key players in the freeware predictive analytics market in no particular order. Click on the button below for a review of the top predictive analytics freeware software solutions.

R

R
2.Predictive Analytics Software API

Google Prediction API, BigML, Microsoft Azure Machine Learning, Blue Yonder, Swift API, Datagami, GraphLab, Data Science Studio, Apigee Insights, Openscoring.io, Intuitics, Anomaly Detective, Zementis, Predixion, Datumbox Machine Learning Framework, PredictionIO, Logical Glue, Ersatz, H2O, Yottamine, Lattice, InsideView, AgilOne, Futurelytics, Fliptop, RelateIQ, Lumiata, Versium LifeData, Indico, INRIX are some of the Tops Predictive Analytics API in no particular order. Click on the button below for a  review of the top predictive analytics software API solutions:

Google Prediction API

Google Prediction API
3.Predictive Analytics Programs

 data science and predictive analytics programs.

You may also like to review the  business analytics programs list:
4.Predictive Lead Scoring Platforms

Lattice Engines, Fliptop, 6sense, Infer, Leadspace, Mintigo, Salesfusion, Versium, Wise, InsideSales, Lead Liaison, SalesPredict, Custora , Televerde, Futurelytics, Fiserv, Angoss KnowledgeSCORE, KXEN, Predixion, EverString are some of the companies who offer predictive lead scoring platforms in no particular order.

Infer

Infer
5.Predictive Pricing Solutions

Blue Yonder Dynamic Pricing , PROS Pricing Analytics, Zilliant MarginMax, Model N, ECOPA Prezzu, Upstream Commerce Dynamic Pricer, Retalon Dynamic Price Management System are some of the Predictive Pricing Solutions in no particular order.

6.Customer Churn, Renew, Upsell, Cross Sell Software Tools

RapidMiner, Marketo, Preact, Google Prediction API, Lattice Engines, Angoss, PROS Sales Optimizer, Fliptop , Alteryx Analytics, Alpine Chorus, 6sense, Gainsight, GraphLab Create, Predixion Insight, KNIME, Bluenose, Actian, RelateIQ, Zilliant, SalesPredict, Infer, AgilOne, Adobe Recommendations are some of the top customer churn, renew, upsell, cross sell software tools.

More Information on Predictive Analysis Process
Predictive Analytics Process Flow

Predictive Analytics Process Flow

For more information of predictive analytics process, please review the overview of  each components in the predictive analytics process: data collection (data mining), data analysis, statistical analysis, predictive modeling and predictive model deployment.


Predictive analytics is the branch of data mining concerned with the prediction of future probabilities and trends. The central element of predictive analytics is the predictor, a variable that can be measured for an individual or other entity to predict future behavior. For example, an insurance company is likely to take into account potential driving safety predictors such as age, gender, and driving record when issuing car insurance policies.


Multiple predictors are combined into a predictive model, which, when subjected to analysis, can be used to forecast future probabilities with an acceptable level of reliability. In predictive modeling, data is collected, a statistical model is formulated, predictions are made and the model is validated (or revised) as additional data becomes available. Predictive analytics are applied to many research areas, including meteorology, security, genetics, economics, and marketing.

Predictive analytics is a subset of data science. Recognition of the uniqueness of predictive analytics illuminates some interesting needs in research as is illustrated by Table 2.
Table 2. Examples of research in predictive analyticsComparative discipline	Dimension of interest	Predictive analytics research (examples)
Relevant	Less relevant
Statistics	Quantitative	Integrating quantitative and qualitative analysis	Improving Lagrange Multiplier tests for autocorrelation
Forecasting	Predicting the future	Using forecasting techniques for evaluating what would have happened under different circumstances	Deriving generalized estimators of seasonal factors
Optimization	Minimization and maximization	Assessment of the quality of the optimal solution and the ability to implement it versus near optimal solutions	Use of polyhedral functions in linear programming
Discrete event simulation	Quantitative analysis of a system in a stochastic setting	Discrete event simulation in a business process reengineering setting	Random number generation for discrete event simulation
Applied probability	Description of stochastic variables, expected values, and uncertainty	Applied probability along with application anchoring and framing affects from psychology	Asymptotic properties of Gaussian processes
Data mining	Search for patterns and relationships between a large number of variables with lots of data	Data mining preceded by logical and theoretical descriptions of possible relationships and patterns	Gibbs posterior for variable selection in data mining
Analytical mathematical modeling	Precise analysis using artificial and unrealistic assumptions for theorems and proofs	Methods of quickly and inexpensively modeling approximate relationships between variables while still using deductive mathematical methods	Proving inventory theorems that assume known, continuous demand with perfect information

Table 2 examines a sample of disciplines related to predictive analytics, selects a dimension of that discipline, and compares possible research topics and provides an example of a research area that would be more relevant to predictive analytics and an example of a research area that would be less relevant. Table 2 indirectly points to the distinction between predictive analytics and each of these quantitative disciplines. It also provides researchers with possible avenues of research that would be in the realm of predictive analytics.

Importantly, although predictive analytics is related to many long-standing quantitative approaches, it stands as distinct from each. Statistics is quantitative, whereas predictive analytics is both quantitative and qualitative. Forecasting is about predicting the future, and predictive analytics adds questions regarding what would have happened in the past, given different conditions. Optimization is about finding the minimum or maximum of a function, subject to constraints, whereas predictive analytics also concerns what would characterize a system that was not operating optimally. Analytical modeling is primarily about generating mathematical axioms and then proving lemmas and theorems, whereas predictive analytics attempts to quickly and inexpensively approximate relationships between variables while still using deductive mathematical methods to draw conclusions. These are some examples of the differences in emphasis between predictive analytics and well known quantitative disciplines.

The topics in Table 2 have been examined in part, but additional research in these relevant areas would advance predictive analytics' ability to refine and improve supply chain decision making. Indeed, the  of Business Logistics is interested in predictive analytics research that is relevant to logistics and SCM. To that end, we propose definitions of logistics and supply chain predictive analytics:

    Logistics predictive analytics use both quantitative and qualitative methods to estimate the past and future behavior of the flow and storage of inventory, as well as the associated costs and service levels.

    SCM predictive analytics use both quantitative and qualitative methods to improve supply chain design and competitiveness by estimating past and future levels of integration of business processes among functions or companies, as well as the associated costs and service levels.

What is defined here as logistics predictive analytics and SCM predictive analytics has already existed in the past, it just lacked a name. The idea is becoming so common that a name helps with communication about the concept. Reading Table 2 with these definitions in mind should provide a guide to appropriate research on logistics or SCM predictive analytics that would be of particular interest at the  of Business Logistics. Barton and Court (2012) highlight the growing value of advanced analytics:

    “Advanced analytics is likely to become a decisive competitive asset in many industries and a core element in companies' efforts to improve performance. It's a mistake to assume that acquiring the right kind of big data is all that matters. Also essential is developing analytics tools that focus on business outcomes … . (p. 81)The concept of big data has been around for years; most organizations now understand that if they capture all the data that streams into their businesses, they can apply analytics and get significant value from it. But even in the 1950s, decades before anyone uttered the term “big data,” businesses were using basic analytics (essentially numbers in a spreadsheet that were manually examined) to uncover insights and trends.

The new benefits that big data analytics brings to the table, however, are speed and efficiency. Whereas a few years ago a business would have gathered information, run analytics and unearthed information that could be used for future decisions, today that business can identify insights for immediate decisions. The ability to work faster – and stay agile – gives organizations a competitive edge they didn’t have before.

 
The Importance of Big Data Analytics Graphic
Why is big data analytics important?

Big data analytics helps organizations harness their data and use it to identify new opportunities. That, in turn, leads to smarter business moves, more efficient operations, higher profits and happier customers. In his report Big Data in Big Companies, IIA Director of Research Tom Davenport interviewed more than 50 businesses to understand how they used big data. He found they got value in the following ways:

    Cost reduction. Big data technologies such as Hadoop and cloud-based analytics bring significant cost advantages when it comes to storing large amounts of data – plus they can identify more efficient ways of doing business.
    Faster, better decision making. With the speed of Hadoop and in-memory analytics, combined with the ability to analyze new sources of data, businesses are able to analyze information immediately – and make decisions based on what they’ve learned.
    New products and services. With the ability to gauge customer needs and satisfaction through analytics comes the power to give customers what they want. Davenport points out that with big data analytics, more companies are creating new products to meet customers’ needs.


Analysis of data is a process of inspecting, cleaning, transforming, and modeling data with the goal of discovering useful information, suggesting conclusions, and supporting decision-making. Data analysis has multiple facets and approaches, encompassing diverse techniques under a variety of names, in different business, science, and social science domains.

Data mining is a particular data analysis technique that focuses on modeling and knowledge discovery for predictive rather than purely descriptive purposes. Business intelligence covers data analysis that relies heavily on aggregation, focusing on business information. In statistical applications, some people divide data analysis into descriptive statistics, exploratory data analysis (EDA), and confirmatory data analysis (CDA). EDA focuses on discovering new features in the data and CDA on confirming or falsifying existing hypotheses. Predictive analytics focuses on application of statistical models for predictive forecasting or classification, while text analytics applies statistical, linguistic, and structural techniques to extract and classify information from textual sources, a species of unstructured data. All are varieties of data analysis.

Data integration is a precursor to data analysis, and data analysis is closely linked to data visualization and data dissemination. The term data analysis is sometimes used as a synonym for data modeling.

Contents

    1 The process of data analysis
        1.1 Data requirements
        1.2 Data collection
        1.3 Data processing
        1.4 Data cleaning
        1.5 Exploratory data analysis
        1.6 Modeling and algorithms
        1.7 Data product
        1.8 Communication
    2 Quantitative messages
    3 Techniques for analyzing quantitative data
    4 Analytical activities of data users
    5 Barriers to effective analysis
        5.1 Confusing fact and opinion
        5.2 Cognitive biases
        5.3 Innumeracy
    6 Other topics
        6.1 Analytics and business intelligence
        6.2 Education
    7 Practitioner notes
        7.1 Initial data analysis
            7.1.1 Quality of data
            7.1.2 Quality of measurements
            7.1.3 Initial transformations
            7.1.4 Did the implementation of the study fulfill the intentions of the research design?
            7.1.5 Characteristics of data sample
            7.1.6 Final stage of the initial data analysis
            7.1.7 Analysis
            7.1.8 Nonlinear analysis
        7.2 Main data analysis
            7.2.1 Exploratory and confirmatory approaches
            7.2.2 Stability of results
            7.2.3 Statistical methods
    8 Free software for data analysis
    9 See also
    10 References
        10.1 Citations
        10.2 Bibliography
    11 Further reading

The process of data analysis
Data science process flowchart

Analysis refers to breaking a whole into its separate components for individual examination. Data analysis is a process for obtaining raw data and converting it into information useful for decision-making by users. Data is collected and analyzed to answer questions, test hypotheses or disprove theories.[1]

Statistician John Tukey defined data analysis in 1961 as: "Procedures for analyzing data, techniques for interpreting the results of such procedures, ways of planning the gathering of data to make its analysis easier, more precise or more accurate, and all the machinery and results of (mathematical) statistics which apply to analyzing data."[2]

There are several phases that can be distinguished, described below. The phases are iterative, in that feedback from later phases may result in additional work in earlier phases.[3]
Data requirements

The data necessary as inputs to the analysis are specified based upon the requirements of those directing the analysis or customers who will use the finished product of the analysis. The general type of entity upon which the data will be collected is referred to as an experimental unit (e.g., a person or population of people). Specific variables regarding a population (e.g., age and income) may be specified and obtained. Data may be numerical or categorical (i.e., a text label for numbers).[3]
Data collection

Data is collected from a variety of sources. The requirements may be communicated by analysts to custodians of the data, such as information technology personnel within an organization. The data may also be collected from sensors in the environment, such as traffic cameras, satellites, recording devices, etc. It may also be obtained through interviews, downloads from online sources, or reading documentation.[3]
Data processing
The phases of the intelligence cycle used to convert raw information into actionable intelligence or knowledge are conceptually similar to the phases in data analysis.

Data initially obtained must be processed or organized for analysis. For instance, this may involve placing data into rows and columns in a table format for further analysis, such as within a spreadsheet or statistical software.[3]
Data cleaning

Once processed and organized, the data may be incomplete, contain duplicates, or contain errors. The need for data cleaning will arise from problems in the way that data is entered and stored. Data cleaning is the process of preventing and correcting these errors. Common tasks include record matching, deduplication, and column segmentation.[4] Such data problems can also be identified through a variety of analytical techniques. For example, with financial information, the totals for particular variables may be compared against separately published numbers believed to be reliable.[5] Unusual amounts above or below pre-determined thresholds may also be reviewed. There are several types of data cleaning that depend on the type of data. Quantitative data methods for outlier detection can be used to get rid of likely incorrectly entered data. Textual data spellcheckers can be used to lessen the amount of mistyped words, but it is harder to tell if the words themselves are correct.[6]
Exploratory data analysis

Once the data is cleaned, it can be analyzed. Analysts may apply a variety of techniques referred to as exploratory data analysis to begin understanding the messages contained in the data.[7][8] The process of exploration may result in additional data cleaning or additional requests for data, so these activities may be iterative in nature. Descriptive statistics such as the average or median may be generated to help understand the data. Data visualization may also be used to examine the data in graphical format, to obtain additional insight regarding the messages within the data.[3]
Modeling and algorithms

Mathematical formulas or models called algorithms may be applied to the data to identify relationships among the variables, such as correlation or causation. In general terms, models may be developed to evaluate a particular variable in the data based on other variable(s) in the data, with some residual error depending on model accuracy (i.e., Data = Model + Error).[1]

Inferential statistics includes techniques to measure relationships between particular variables. For example, regression analysis may be used to model whether a change in advertising (independent variable X) explains the variation in sales (dependent variable Y). In mathematical terms, Y (sales) is a function of X (advertising). It may be described as Y = aX + b + error, where the model is designed such that a and b minimize the error when the model predicts Y for a given range of values of X. Analysts may attempt to build models that are descriptive of the data to simplify analysis and communicate results.[1]
Data product

A data product is a computer application that takes data inputs and generates outputs, feeding them back into the environment. It may be based on a model or algorithm. An example is an application that analyzes data about customer purchasing history and recommends other purchases the customer might enjoy.[3]
Communication
Data visualization to understand the results of a data analysis.[9]
Main article: Data visualization

Once the data is analyzed, it may be reported in many formats to the users of the analysis to support their requirements. The users may have feedback, which results in additional analysis. As such, much of the analytical cycle is iterative.[3]

When determining how to communicate the results, the analyst may consider data visualization techniques to help clearly and efficiently communicate the message to the audience. Data visualization uses information displays such as tables and charts to help communicate key messages contained in the data. Tables are helpful to a user who might lookup specific numbers, while charts (e.g., bar charts or line charts) may help explain the quantitative messages contained in the data.
Quantitative messages
Main article: Data visualization
A time series illustrated with a line chart demonstrating trends in U.S. federal spending and revenue over time.
A scatterplot illustrating correlation between two variables (inflation and unemployment) measured at points in time.

Author Stephen Few described eight types of quantitative messages that users may attempt to understand or communicate from a set of data and the associated graphs used to help communicate the message. Customers specifying requirements and analysts performing the data analysis may consider these messages during the course of the process.

    Time-series: A single variable is captured over a period of time, such as the unemployment rate over a 10-year period. A line chart may be used to demonstrate the trend.
    Ranking: Categorical subdivisions are ranked in ascending or descending order, such as a ranking of sales performance (the measure) by sales persons (the category, with each sales person a categorical subdivision) during a single period. A bar chart may be used to show the comparison across the sales persons.
    Part-to-whole: Categorical subdivisions are measured as a ratio to the whole (i.e., a percentage out of 100%). A pie chart or bar chart can show the comparison of ratios, such as the market share represented by competitors in a market.
    Deviation: Categorical subdivisions are compared against a reference, such as a comparison of actual vs. budget expenses for several departments of a business for a given time period. A bar chart can show comparison of the actual versus the reference amount.
    Frequency distribution: Shows the number of observations of a particular variable for given interval, such as the number of years in which the stock market return is between intervals such as 0-10%, 11-20%, etc. A histogram, a type of bar chart, may be used for this analysis.
    Correlation: Comparison between observations represented by two variables (X,Y) to determine if they tend to move in the same or opposite directions. For example, plotting unemployment (X) and inflation (Y) for a sample of months. A scatter plot is typically used for this message.
    Nominal comparison: Comparing categorical subdivisions in no particular order, such as the sales volume by product code. A bar chart may be used for this comparison.
    Geographic or geospatial: Comparison of a variable across a map or layout, such as the unemployment rate by state or the number of persons on the various floors of a building. A cartogram is a typical graphic used.[10][11]

Techniques for analyzing quantitative data
See also: Problem solving

Author Jonathan Koomey has recommended a series of best practices for understanding quantitative data. These include:

    Check raw data for anomalies prior to performing your analysis;
    Re-perform important calculations, such as verifying columns of data that are formula driven;
    Confirm main totals are the sum of subtotals;
    Check relationships between numbers that should be related in a predictable way, such as ratios over time;
    Normalize numbers to make comparisons easier, such as analyzing amounts per person or relative to GDP or as an index value relative to a base year;
    Break problems into component parts by analyzing factors that led to the results, such as DuPont analysis of return on equity.[5]

For the variables under examination, analysts typically obtain descriptive statistics for them, such as the mean (average), median, and standard deviation. They may also analyze the distribution of the key variables to see how the individual values cluster around the mean.
An illustration of the MECE principle used for data analysis.

The consultants at McKinsey and Company named a technique for breaking a quantitative problem down into its component parts called the MECE principle. Each layer can be broken down into its components; each of the sub-components must be mutually exclusive of each other and collectively add up to the layer above them. The relationship is referred to as "Mutually Exclusive and Collectively Exhaustive" or MECE. For example, profit by definition can be broken down into total revenue and total cost. In turn, total revenue can be analyzed by its components, such as revenue of divisions A, B, and C (which are mutually exclusive of each other) and should add to the total revenue (collectively exhaustive).

Analysts may use robust statistical measurements to solve certain analytical problems. Hypothesis testing is used when a particular hypothesis about the true state of affairs is made by the analyst and data is gathered to determine whether that state of affairs is true or false. For example, the hypothesis might be that "Unemployment has no effect on inflation", which relates to an economics concept called the Phillips Curve. Hypothesis testing involves considering the likelihood of Type I and type II errors, which relate to whether the data supports accepting or rejecting the hypothesis.

Regression analysis may be used when the analyst is trying to determine the extent to which independent variable X affects dependent variable Y (e.g., "To what extent do changes in the unemployment rate (X) affect the inflation rate (Y)?"). This is an attempt to model or fit an equation line or curve to the data, such that Y is a function of X.

Necessary condition analysis 
(NCA) may be used when the analyst is trying to determine the extent to which independent variable X allows variable Y (e.g., "To what extent is a certain unemployment rate (X) necessary for a certain inflation rate (Y)?"). Whereas (multiple) regression analysis uses additive logic where each X-variable can produce the outcome and the X's can compensate for each other (they are sufficient but not necessary), necessary condition analysis (NCA) uses necessity logic, where one or more X-variables allow the outcome to exist, but may not produce it (they are necessary but not sufficient). Each single necessary condition must be present and compensation is not possible.
Analytical activities of data users

Users may have particular data points of interest within a data set, as opposed to general messaging outlined above. Such low-level user analytic activities are presented in the following table. The taxonomy can also be organized by three poles of activities: retrieving values, finding data points, and arranging data points.[12][13][14]
# 	Task 	General
Description 	Pro Forma
Abstract 	Examples
1 	Retrieve Value 	Given a set of specific cases, find attributes of those cases. 	What are the values of attributes {X, Y, Z, ...} in the data cases {A, B, C, ...}? 	- What is the mileage per gallon of the Audi TT?

- How long is the movie Gone with the Wind?
2 	Filter 	Given some concrete conditions on attribute values, find data cases satisfying those conditions. 	Which data cases satisfy conditions {A, B, C...}? 	- What Kellogg's cereals have high fiber?

- What comedies have won awards?

- Which funds underperformed the SP-500?
3 	Compute Derived Value 	Given a set of data cases, compute an aggregate numeric representation of those data cases. 	What is the value of aggregation function F over a given set S of data cases? 	- What is the average calorie content of Post cereals?

- What is the gross income of all stores combined?

- How many manufacturers of cars are there?
4 	Find Extremum 	Find data cases possessing an extreme value of an attribute over its range within the data set. 	What are the top/bottom N data cases with respect to attribute A? 	- What is the car with the highest MPG?

- What director/film has won the most awards?

- What Robin Williams film has the most recent release date?
5 	Sort 	Given a set of data cases, rank them according to some ordinal metric. 	What is the sorted order of a set S of data cases according to their value of attribute A? 	- Order the cars by weight.

- Rank the cereals by calories.
6 	Determine Range 	Given a set of data cases and an attribute of interest, find the span of values within the set. 	What is the range of values of attribute A in a set S of data cases? 	- What is the range of film lengths?

- What is the range of car horsepowers?

- What actresses are in the data set?
7 	Characterize Distribution 	Given a set of data cases and a quantitative attribute of interest, characterize the distribution of that attribute’s values over the set. 	What is the distribution of values of attribute A in a set S of data cases? 	- What is the distribution of carbohydrates in cereals?

- What is the age distribution of shoppers?
8 	Find Anomalies 	Identify any anomalies within a given set of data cases with respect to a given relationship or expectation, e.g. statistical outliers. 	Which data cases in a set S of data cases have unexpected/exceptional values? 	- Are there exceptions to the relationship between horsepower and acceleration?

- Are there any outliers in protein?
9 	Cluster 	Given a set of data cases, find clusters of similar attribute values. 	Which data cases in a set S of data cases are similar in value for attributes {X, Y, Z, ...}? 	- Are there groups of cereals w/ similar fat/calories/sugar?

- Is there a cluster of typical film lengths?
10 	Correlate 	Given a set of data cases and two attributes, determine useful relationships between the values of those attributes. 	What is the correlation between attributes X and Y over a given set S of data cases? 	- Is there a correlation between carbohydrates and fat?

- Is there a correlation between country of origin and MPG?

- Do different genders have a preferred payment method?

- Is there a trend of increasing film length over the years?
Barriers to effective analysis

Barriers to effective analysis may exist among the analysts performing the data analysis or among the audience. Distinguishing fact from opinion, cognitive biases, and innumeracy are all challenges to sound data analysis.
Confusing fact and opinion

You are entitled to your own opinion, but you are not entitled to your own facts.
Daniel Patrick Moynihan

Effective analysis requires obtaining relevant facts to answer questions, support a conclusion or formal opinion, or test hypotheses. Facts by definition are irrefutable, meaning that any person involved in the analysis should be able to agree upon them. For example, in August 2010, the Congressional Budget Office (CBO) estimated that extending the Bush tax cuts of 2001 and 2003 for the 2011-2020 time period would add approximately $3.3 trillion to the national debt.[15] Everyone should be able to agree that indeed this is what CBO reported; they can all examine the report. This makes it a fact. Whether persons agree or disagree with the CBO is their own opinion.

As another example, the auditor of a public company must arrive at a formal opinion on whether financial statements of publicly traded corporations are "fairly stated, in all material respects." This requires extensive analysis of factual data and evidence to support their opinion. When making the leap from facts to opinions, there is always the possibility that the opinion is erroneous.
Cognitive biases

There are a variety of cognitive biases that can adversely effect analysis. For example, confirmation bias is the tendency to search for or interpret information in a way that confirms one's preconceptions. In addition, individuals may discredit information that does not support their views.

Analysts may be trained specifically to be aware of these biases and how to overcome them. In his book Psychology of Intelligence Analysis, retired CIA analyst Richards Heuer wrote that analysts should clearly delineate their assumptions and chains of inference and specify the degree and source of the uncertainty involved in the conclusions. He emphasized procedures to help surface and debate alternative points of view.[16]
Innumeracy

Effective analysts are generally adept with a variety of numerical techniques. However, audiences may not have such literacy with numbers or numeracy; they are said to be innumerate. Persons communicating the data may also be attempting to mislead or misinform, deliberately using bad numerical techniques.[17]

For example, whether a number is rising or falling may not be the key factor. More important may be the number relative to another number, such as the size of government revenue or spending relative to the size of the economy (GDP) or the amount of cost relative to revenue in corporate financial statements. This numerical technique is referred to as normalization[5] or common-sizing. There are many such techniques employed by analysts, whether adjusting for inflation (i.e., comparing real vs. nominal data) or considering population increases, demographics, etc. Analysts apply a variety of techniques to address the various quantitative messages described in the section above.

Analysts may also analyze data under different assumptions or scenarios. For example, when analysts perform financial statement analysis, they will often recast the financial statements under different assumptions to help arrive at an estimate of future cash flow, which they then discount to present value based on some interest rate, to determine the valuation of the company or its stock. Similarly, the CBO analyzes the effects of various policy options on the government's revenue, outlays and deficits, creating alternative future scenarios for key measures.
Other topics
Analytics and business intelligence
Main article: Analytics

Analytics is the "extensive use of data, statistical and quantitative analysis, explanatory and predictive models, and fact-based management to drive decisions and actions." It is a subset of business intelligence, which is a set of technologies and processes that use data to understand and analyze business performance.[18]
Education
Analytic activities of data visualization users

In education, most educators have access to a data system for the purpose of analyzing student data.[19] These data systems present data to educators in an over-the-counter data format (embedding labels, supplemental documentation, and a help system and making key package/display and content decisions) to improve the accuracy of educators’ data analyses.[20]
Practitioner notes

This section contains rather technical explanations that may assist practitioners but are beyond the typical scope of a Wikipedia article.
Initial data analysis

The most important distinction between the initial data analysis phase and the main analysis phase, is that during initial data analysis one refrains from any analysis that is aimed at answering the original research question. The initial data analysis phase is guided by the following four questions:[21]
Quality of data

The quality of the data should be checked as early as possible. Data quality can be assessed in several ways, using different types of analysis: frequency counts, descriptive statistics (mean, standard deviation, median), normality (skewness, kurtosis, frequency histograms, n: variables are compared with coding schemes of variables external to the data set, and possibly corrected if coding schemes are not comparable.

    Test for common-method variance.

The choice of analyses to assess the data quality during the initial data analysis phase depends on the analyses that will be conducted in the main analysis phase.[22]
Quality of measurements

The quality of the measurement instruments should only be checked during the initial data analysis phase when this is not the focus or research question of the study. One should check whether structure of measurement instruments corresponds to structure reported in the literature.
There are two ways to assess measurement

    Analysis of homogeneity (internal consistency), which gives an indication of the reliability of a measurement instrument. During this analysis, one inspects the variances of the items and the scales, the Cronbach's α of the scales, and the change in the Cronbach's alpha when an item would be deleted from a scale.[23]

Initial transformations

After assessing the quality of the data and of the measurements, one might decide to impute missing data, or to perform initial transformations of one or more variables, although this can also be done during the main analysis phase.[24]
Possible transformations of variables are:[25]

    Square root transformation (if the distribution differs moderately from normal)
    Log-transformation (if the distribution differs substantially from normal)
    Inverse transformation (if the distribution differs severely from normal)
    Make categorical (ordinal / dichotomous) (if the distribution differs severely from normal, and no transformations help)

Did the implementation of the study fulfill the intentions of the research design?

One should check the success of the randomization procedure, for instance by checking whether background and substantive variables are equally distributed within and across groups.
If the study did not need or use a randomization procedure, one should check the success of the non-random sampling, for instance by checking whether all subgroups of the population of interest are represented in sample.
Other possible data distortions that should be checked are:

    dropout (this should be identified during the initial data analysis phase)
    Item nonresponse (whether this is random or not should be assessed during the initial data analysis phase)
    Treatment quality (using manipulation checks).[26]

Characteristics of data sample

In any report or article, the structure of the sample must be accurately described. It is especially important to exactly determine the structure of the sample (and specifically the size of the subgroups) when subgroup analyses will be performed during the main analysis phase.
The characteristics of the data sample can be assessed by looking at:

    Basic statistics of important variables
    Scatter plots
    Correlations and associations
    Cross-tabulations[27]

Final stage of the initial data analysis

During the final stage, the findings of the initial data analysis are documented, and necessary, preferable, and possible corrective actions are taken.
Also, the original plan for the main data analyses can and should be specified in more detail or rewritten.
In order to do this, several decisions about the main data analyses can and should be made:

    In the case of non-normals: should one transform variables; make variables categorical (ordinal/dichotomous); adapt the analysis method?
    In the case of missing data: should one neglect or impute the missing data; which imputation technique should be used?
    In the case of outliers: should one use robust analysis techniques?
    In case items do not fit the scale: should one adapt the measurement instrument by omitting items, or rather ensure comparability with other (uses of the) measurement instrument(s)?
    In the case of (too) small subgroups: should one drop the hypothesis about inter-group differences, or use small sample techniques, like exact tests or bootstrapping?
    In case the randomization procedure seems to be defective: can and should one calculate propensity scores and include them as covariates in the main analyses?[28]

Analysis

Several analyses can be used during the initial data analysis phase:[29]

    Univariate statistics (single variable)
    Bivariate associations (correlations)
    Graphical techniques (scatter plots)

It is important to take the measurement levels of the variables into account for the analyses, as special statistical techniques are available for each level:[30]

    Nominal and ordinal variables
        Frequency counts (numbers and percentages)
        Associations
            circumambulations (crosstabulations)
            hierarchical loglinear analysis (restricted to a maximum of 8 variables)
            loglinear analysis (to identify relevant/important variables and possible confounders)
        Exact tests or bootstrapping (in case subgroups are small)
        Computation of new variables
    Continuous variables
        Distribution
            Statistics (M, SD, variance, skewness, kurtosis)
            Stem-and-leaf displays
            Box plots

Nonlinear analysis

Nonlinear analysis will be necessary when the data is recorded from a nonlinear system. Nonlinear systems can exhibit complex dynamic effects including bifurcations, chaos, harmonics and subharmonics that cannot be analyzed using simple linear methods. Nonlinear data analysis is closely related to nonlinear system identification.[31]
Main data analysis

In the main analysis phase analyses aimed at answering the research question are performed as well as any other relevant analysis needed to write the first draft of the research report.[32]
Exploratory and confirmatory approaches

In the main analysis phase either an exploratory or confirmatory approach can be adopted. Usually the approach is decided before data is collected. In an exploratory analysis no clear hypothesis is stated before analysing the data, and the data is searched for models that describe the data well. In a confirmatory analysis clear hypotheses about the data are tested.

Exploratory data analysis should be interpreted carefully. When testing multiple models at once there is a high chance on finding at least one of them to be significant, but this can be due to a type 1 error. It is important to always adjust the significance level when testing multiple models with, for example, a Bonferroni correction. Also, one should not follow up an exploratory analysis with a confirmatory analysis in the same dataset. An exploratory analysis is used to find ideas for a theory, but not to test that theory as well. When a model is found exploratory in a dataset, then following up that analysis with a confirmatory analysis in the same dataset could simply mean that the results of the confirmatory analysis are due to the same type 1 error that resulted in the exploratory model in the first place. The confirmatory analysis therefore will not be more informative than the original exploratory analysis.[33]
Stability of results

It is important to obtain some indication about how generalizable the results are.[34] While this is hard to check, one can look at the stability of the results. Are the results reliable and reproducible? There are two main ways of doing this:

    Cross-validation: By splitting the data in multiple parts we can check if an analysis (like a fitted model) based on one part of the data generalizes to another part of the data as well.
    Sensitivity analysis: A procedure to study the behavior of a system or model when global parameters are (systematically) varied. One way to do this is with bootstrapping.

Statistical methods

Many statistical methods have been used for statistical analyses. A very brief list of four of the more popular methods is:

    General linear model: A widely used model on which various methods are based (e.g. t test, ANOVA, ANCOVA, MANOVA). Usable for assessing the effect of several predictors on one or more continuous dependent variables.
    Generalized linear model: An extension of the general linear model for discrete dependent variables.
    Structural equation modelling: Usable for assessing latent structures from measured manifest variables.
    Item response theory: Models for (mostly) assessing one latent variable from several binary measured variables (e.g. an exam).

Free software for data analysis

    NCA Calculator 
    - a simple online calculator for finding necessary but not sufficient conditions in datasets
    NCA Software 
    - R package for finding necessary but not sufficient conditions in datasets
    Data Applied - an online data mining and data visualization solution.
    DataMelt - a multiplatform (Java-based) data analysis framework from the jWork.ORG 
    community of developers led by Dr. S.Chekanov
    DevInfo - a database system endorsed by the United Nations Development Group for monitoring and analyzing human development.
    ELKI - data mining framework in Java with data mining oriented visualization functions.
    KNIME - the Konstanz Information Miner, a user friendly and comprehensive data analytics framework.
    MEPX 
    - cross platform tool for regression and classification problems.
    PAW - FORTRAN/C data analysis framework developed at CERN
    Orange - A visual programming tool featuring interactive data visualization and methods for statistical data analysis, data mining, and machine learning.
    QSoas - An open source, command-driven program for analyzing y=f(x) data (noise removal, baseline corrections, global fitting the solutions of differential equations or kinetic schemes, and more). Binaries available for Mac OSX and Windows. http://www.qsoas.org 
    [35]
    R - a programming language and software environment for statistical computing and graphics.
    ROOT - C++ data analysis framework developed at CERN
    dotplot - cloud based visual designer to create analytic models[36]
    SciPy - A set of Python tools for data analysis http://scipy.org/stackspec.html 
    Statsmodels - a Python module that allows users to explore data, estimate statistical models, and perform statistical tests http://statsmodels.sourceforge.net/ 
    Pandas - A software library written for the Python programming language for data manipulation and analysis.
    myInvenio [37]- a cloud based solution to automatically discover processes from event logs.

See also
Portal icon 	statistics portal

    Analytics
    Business intelligence
    Censoring (statistics)
    Computational physics
    Data acquisition
    Data governance
    Data mining
    Data Presentation Architecture
    Data science
    Digital signal processing
    Dimension reduction
    Early case assessment
    Exploratory data analysis
    Fourier analysis
    Machine learning
    Multilinear PCA
    Multilinear subspace learning
    Multiway Data Analysis
    Nearest neighbor search
    nonlinear system identification
    Predictive analytics
    Principal component analysis
    Qualitative research
    Scientific computing
    Structured data analysis (statistics)
    system identification
    Test method
    Text analytics
    Unstructured data
    Wavelet






